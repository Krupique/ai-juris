{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Juris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import shutil\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes folders if they already exist. This avoids errors when running jupyter from the second time onwards\n",
    "try:\n",
    "    shutil.rmtree('train_logs')\n",
    "    shutil.rmtree('train_results')\n",
    "    shutil.rmtree('saved_model')\n",
    "except:\n",
    "    print('The folders do not exist or have already been removed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name\n",
    "filename = 'data/dataset.csv'\n",
    "\n",
    "# Load data\n",
    "dataset = load_dataset('csv', data_files=filename)\n",
    "\n",
    "# Splitting into training and testint with 80/20 ratio\n",
    "dataset = dataset['train'].train_test_split(test_size = 0.2)\n",
    "\n",
    "# Show dataset format\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer and LLM Open-Source\n",
    "\n",
    "https://huggingface.co/google/flan-t5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "# Showing the tokenizer\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained LLM\n",
    "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "# Show the model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator to concatenate the tokenizer and the model\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Show the Data Collator\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every input will receive the prefix: \"answer the question\"\n",
    "prefix = \"answer the question: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def data_preprocess(data):\n",
    "    # Concatenate the prefix to each question in the list of questions given in data[\"question\"]\n",
    "    inputs = [prefix + doc for doc in data['question']]\n",
    "\n",
    "    # Uses the tokenizer to convert the processed questions into tokens with a maximum lenght of 128, truncating any that are longer\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    # Tokenize the responses given in data['answer] with a maximum lenght of 512, truncating any that are longer\n",
    "    labels = tokenizer(text_target = data['answer'], max_length=512, truncation=True)\n",
    "\n",
    "    # Add the tokens of response as labels in the input dictionary of the model\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the preprocessing function to the dataset, generating the tokenized dataset \n",
    "dataset_tokenized = dataset.map(data_preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the dataset tokenized\n",
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized['train']['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized['train']['answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Evaluate Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"punkt\" package is specifically for the task of tokenization, which involves splitting a text\n",
    "# into a list of sentences\n",
    "nltk.download(\"punkt\", quiet = True)\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ROUGE** metric (*Recall-Oriented Understudy for Gisting Evaluation*) is widely used to automatically evaluate the quality of machine-generated text summaries by comparing them to human-written reference summaries. It measures the overlap between n-grams, words, or sequences of words in the generated text and the reference text.\n",
    "\n",
    "### Common ROUGE Variants\n",
    "\n",
    "1. **ROUGE-N**:\n",
    "   - Measures the overlap of n-grams between the generated and reference texts.\n",
    "   - Example: ROUGE-1 (for unigrams), ROUGE-2 (for bigrams), etc.\n",
    "   - Formula:\n",
    "     \n",
    "     $\\text{ROUGE-N} = \\frac{\\sum_{S \\in \\text{Reference}} \\sum_{\\text{n-gram} \\in S} \\text{Count\\_overlap}(\\text{n-gram})}{\\sum_{S \\in \\text{Reference}} \\sum_{\\text{n-gram} \\in S} \\text{Count}(\\text{n-gram})}$\n",
    "     \n",
    "\n",
    "2. **ROUGE-L**:\n",
    "   - Based on the *Longest Common Subsequence* (LCS), measuring the longest sequence of words that appears in both the generated and reference texts.\n",
    "   - Useful because it accounts for word order without requiring the words to be contiguous.\n",
    "\n",
    "3. **ROUGE-W**:\n",
    "   - A variation of ROUGE-L that assigns weights to continuous subsequences, giving more importance to longer segments.\n",
    "\n",
    "4. **ROUGE-S** (or ROUGE-Skip):\n",
    "   - Measures co-occurrences of word pairs that appear in the same order but may be separated by other words.\n",
    "\n",
    "5. **ROUGE-SU**:\n",
    "   - Combines ROUGE-S with unigrams, adding more context to the evaluation.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "- **Recall**:\n",
    "  - Measures how much of the reference text is captured in the generated text.\n",
    "  - Useful for summarization tasks, as it prioritizes capturing essential information.\n",
    "\n",
    "- **Precision**:\n",
    "  - Measures how much of the generated text is present in the reference.\n",
    "  - Less commonly emphasized in ROUGE but still relevant.\n",
    "\n",
    "- **F1-Score**:\n",
    "  - Combines *Recall* and *Precision* to provide a balanced metric.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Evaluating text summarization models.\n",
    "- Comparing generated texts in tasks like machine translation, captioning, or automated responses.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider the reference summary:  \n",
    "**\"The cat is on the roof\"**  \n",
    "Machine-generated summary:  \n",
    "**\"The cat sleeps on the roof\"**\n",
    "\n",
    "- **ROUGE-1 (Unigrams)**:  \n",
    "  - Unigrams in the reference: {The, cat, is, on, roof}  \n",
    "  - Unigrams in the generated text: {The, cat, sleeps, on, roof}  \n",
    "  - Overlap: {The, cat, on, roof}  \n",
    "  - Recall = 4/5 = 0.8 (80%)  \n",
    "  - Precision = 4/5 = 0.8 (80%)  \n",
    "  - F1-Score = 0.8 (80%)\n",
    "\n",
    "- **ROUGE-2 (Bigrams)**:  \n",
    "  - Bigrams in the reference: {The cat, cat is, is on, on the roof}  \n",
    "  - Bigrams in the generated text: {The cat, cat sleeps, sleeps on, on the roof}  \n",
    "  - Overlap: {The cat, on the roof}  \n",
    "  - Recall = 2/4 = 0.5 (50%)  \n",
    "  - Precision = 2/4 = 0.5 (50%)  \n",
    "  - F1-Score = 0.5 (50%)\n",
    "\n",
    "ROUGE is useful for automatically evaluating the quality of texts but does not capture semantic or creative nuances. Therefore, it is recommended as a complement to human evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the metric\n",
    "metric = evaluate.load('rouge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric calculate function\n",
    "def calculate_metric(eval_preds):\n",
    "\n",
    "    # Unpack the predictions and labels from the eval_preds argument\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    # Replace all non--100 values ​​in labels with the padding token ID\n",
    "    labels = np.where(labels != -100,\n",
    "                      labels,\n",
    "                      tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode predictions to text, ignoring special tokens\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Decode labels to text, ignoring special tokens\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Add a new line after each sentence to the decoded predictions, preparing them for ROUGE evaluation\n",
    "    decoded_predictions = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions]\n",
    "    \n",
    "    # Add a new line after each label to the decoded predictions, preparing them for ROUGE evaluation\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "\n",
    "    # Calculate the ROUGE metric between predictions and decoded labels, using a stemmer\n",
    "    result = metric.compute(predictions = decoded_predictions,\n",
    "                            references = decoded_labels,\n",
    "                            use_stemmer = True)\n",
    "    \n",
    "    # Returns the result of ROUGE metric\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`Seq2SeqTrainingArguments`** class from the `transformers` package is used to configure training parameters when fine-tuning sequence-to-sequence (Seq2Seq) models, such as those used in tasks like machine translation, text summarization, or conditional text generation.\n",
    "\n",
    "This class extends the base `TrainingArguments` class and adds specific arguments tailored for Seq2Seq model training, such as those based on **T5**, **BART**, and others.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Arguments**\n",
    "Here are the most relevant arguments:\n",
    "\n",
    "#### **General Arguments** (inherited from `TrainingArguments`):\n",
    "1. **`output_dir`**:\n",
    "   - Directory where models and checkpoints will be saved.\n",
    "   - Example: `output_dir=\"./results\"`\n",
    "\n",
    "2. **`evaluation_strategy`**:\n",
    "   - Strategy for evaluation during training. Options:\n",
    "     - `no`: No evaluation.\n",
    "     - `steps`: Evaluation at specific step intervals.\n",
    "     - `epoch`: Evaluation at the end of each epoch.\n",
    "\n",
    "3. **`per_device_train_batch_size`**:\n",
    "   - Batch size used for training on each device (CPU/GPU).\n",
    "\n",
    "4. **`learning_rate`**:\n",
    "   - Initial learning rate.\n",
    "\n",
    "5. **`num_train_epochs`**:\n",
    "   - Total number of training epochs.\n",
    "\n",
    "6. **`save_steps`**:\n",
    "   - Number of steps between each checkpoint save.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Seq2Seq-Specific Arguments**:\n",
    "1. **`predict_with_generate`**:\n",
    "   - Type: `bool`\n",
    "   - Indicates whether to use the `generate()` method to make predictions during evaluation.\n",
    "   - Very useful for tasks like translation or summarization, where the output is a generated sequence.\n",
    "\n",
    "2. **`generation_max_length`**:\n",
    "   - Type: `int`\n",
    "   - Maximum length of sequences generated during evaluation or prediction.\n",
    "\n",
    "3. **`generation_num_beams`**:\n",
    "   - Type: `int`\n",
    "   - Sets the number of beams used in beam search for sequence generation.\n",
    "   - Example: Setting it to `4` can improve the quality of generated text.\n",
    "\n",
    "4. **`label_smoothing_factor`**:\n",
    "   - Type: `float`\n",
    "   - Label smoothing factor used to prevent overfitting.\n",
    "   - Example: A value like `0.1` smooths the target probabilities.\n",
    "\n",
    "5. **`forced_bos_token_id` and `forced_eos_token_id`**:\n",
    "   - IDs of special tokens to enforce as the beginning (`BOS`) or end (`EOS`) of the generated sequence.\n",
    "   - Useful in scenarios where greater control over the model's output is required.\n",
    "\n",
    "6. **`length_penalty`**:\n",
    "   - Type: `float`\n",
    "   - Penalizes or rewards longer sequences during generation.\n",
    "   - Values less than 1 favor shorter sequences; values greater than 1 favor longer ones.\n",
    "\n",
    "---\n",
    "\n",
    "### **Usage Example**\n",
    "Here’s an example of how to set up arguments for Seq2Seq training:\n",
    "\n",
    "```python\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=4,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use**\n",
    "The **`Seq2SeqTrainingArguments`** class is essential when training Seq2Seq models using the `Trainer` provided by the `transformers` package. It offers a standardized interface to configure both general training aspects and specifics of text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train arguments\n",
    "training_args = Seq2SeqTrainingArguments(output_dir = \"train_results\",\n",
    "                                        evaluation_strategy = \"epoch\",\n",
    "                                        learning_rate = 3e-4,\n",
    "                                        logging_dir = \"logs_treino\",\n",
    "                                        logging_steps = 1,\n",
    "                                        per_device_train_batch_size = 4,\n",
    "                                        per_device_eval_batch_size = 2,\n",
    "                                        weight_decay = 0.01,\n",
    "                                        save_total_limit = 1,\n",
    "                                        num_train_epochs = 1,\n",
    "                                        predict_with_generate = True,\n",
    "                                        push_to_hub = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`Seq2SeqTrainer`** class from the `transformers` package is a specialized version of the `Trainer` class designed specifically for training sequence-to-sequence (Seq2Seq) models. It is tailored for tasks like machine translation, text summarization, and conditional text generation, where both the input and output are sequences.\n",
    "\n",
    "This class simplifies the training and evaluation of Seq2Seq models by integrating features specific to generation tasks, such as beam search, sequence length constraints, and evaluation with the `generate` method.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Features of `Seq2SeqTrainer`**\n",
    "1. **Integration with Generation**:\n",
    "   - Uses the model's `generate()` method for prediction during evaluation or inference, allowing evaluation metrics to be computed on generated sequences.\n",
    "\n",
    "2. **Support for Seq2Seq-Specific Metrics**:\n",
    "   - Metrics like ROUGE, BLEU, and others that rely on generated text can be directly integrated into the evaluation pipeline.\n",
    "\n",
    "3. **Handles Forced Tokens**:\n",
    "   - Supports forcing specific tokens (e.g., BOS/EOS tokens) at the start or end of generated sequences using arguments like `forced_bos_token_id` and `forced_eos_token_id`.\n",
    "\n",
    "4. **Extended Arguments**:\n",
    "   - Works seamlessly with `Seq2SeqTrainingArguments`, which includes additional parameters like `generation_max_length`, `generation_num_beams`, and `label_smoothing_factor`.\n",
    "\n",
    "5. **Label Smoothing**:\n",
    "   - Implements label smoothing during training to make the model more robust and prevent overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Methods**\n",
    "#### 1. **`compute_loss`**:\n",
    "   - Computes the loss during training, optionally applying label smoothing if configured.\n",
    "\n",
    "#### 2. **`prediction_step`**:\n",
    "   - Overrides the base `Trainer`'s method to support predictions using `generate()` for Seq2Seq tasks.\n",
    "\n",
    "#### 3. **`evaluate`**:\n",
    "   - Evaluates the model using generated sequences instead of raw logits.\n",
    "   - Automatically applies `generation_max_length` and `generation_num_beams` for evaluation.\n",
    "\n",
    "#### 4. **`generate`**:\n",
    "   - Handles sequence generation using the model's `generate()` method, with support for various generation strategies like greedy search, beam search, or sampling.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Configuration Parameters**\n",
    "`Seq2SeqTrainer` inherits all parameters from `Trainer` and adds Seq2Seq-specific ones:\n",
    "\n",
    "1. **`tokenizer`**:\n",
    "   - Used to tokenize the input and decode generated sequences.\n",
    "\n",
    "2. **`predict_with_generate`**:\n",
    "   - Enables the use of the model's `generate()` method during evaluation.\n",
    "\n",
    "3. **`generation_max_length`**:\n",
    "   - Maximum length for generated sequences during evaluation or prediction.\n",
    "\n",
    "4. **`generation_num_beams`**:\n",
    "   - Number of beams for beam search in sequence generation.\n",
    "\n",
    "5. **`label_smoothing_factor`**:\n",
    "   - Factor for label smoothing during training.\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use**\n",
    "- Use `Seq2SeqTrainer` when training models for tasks where both the input and output are sequences, and you need additional support for generation and sequence-based metrics.\n",
    "- It is particularly suited for models like **T5**, **BART**, and **MBart**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the trainer\n",
    "trainer = Seq2SeqTrainer(model = model,\n",
    "                        args = training_args,\n",
    "                        train_dataset = dataset_tokenized[\"train\"],\n",
    "                        eval_dataset = dataset_tokenized[\"test\"],\n",
    "                        tokenizer = tokenizer,\n",
    "                        data_collator = data_collator,\n",
    "                        compute_metrics = calculate_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f855402191145f5acfd3fd346aa6993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2324, 'grad_norm': 2.2708263397216797, 'learning_rate': 0.0002995994659546061, 'epoch': 0.0}\n",
      "{'loss': 1.9946, 'grad_norm': 1.7159390449523926, 'learning_rate': 0.00029919893190921226, 'epoch': 0.0}\n",
      "{'loss': 2.5341, 'grad_norm': 2.3515491485595703, 'learning_rate': 0.0002987983978638184, 'epoch': 0.0}\n",
      "{'loss': 2.0063, 'grad_norm': 1.8315774202346802, 'learning_rate': 0.00029839786381842456, 'epoch': 0.01}\n",
      "{'loss': 1.8856, 'grad_norm': 1.7520458698272705, 'learning_rate': 0.0002979973297730307, 'epoch': 0.01}\n",
      "{'loss': 1.8455, 'grad_norm': 1.8115766048431396, 'learning_rate': 0.00029759679572763685, 'epoch': 0.01}\n",
      "{'loss': 2.3088, 'grad_norm': 1.5589631795883179, 'learning_rate': 0.00029719626168224294, 'epoch': 0.01}\n",
      "{'loss': 1.7683, 'grad_norm': 2.0201447010040283, 'learning_rate': 0.0002967957276368491, 'epoch': 0.01}\n",
      "{'loss': 1.8047, 'grad_norm': 1.7122702598571777, 'learning_rate': 0.00029639519359145523, 'epoch': 0.01}\n",
      "{'loss': 2.1273, 'grad_norm': 1.5213422775268555, 'learning_rate': 0.0002959946595460614, 'epoch': 0.01}\n",
      "{'loss': 1.7814, 'grad_norm': 1.8124181032180786, 'learning_rate': 0.0002955941255006675, 'epoch': 0.01}\n",
      "{'loss': 1.7026, 'grad_norm': 1.684412956237793, 'learning_rate': 0.00029519359145527366, 'epoch': 0.02}\n",
      "{'loss': 2.041, 'grad_norm': 1.780172348022461, 'learning_rate': 0.0002947930574098798, 'epoch': 0.02}\n",
      "{'loss': 1.8238, 'grad_norm': 1.4454736709594727, 'learning_rate': 0.00029439252336448596, 'epoch': 0.02}\n",
      "{'loss': 2.5184, 'grad_norm': 1.6748665571212769, 'learning_rate': 0.0002939919893190921, 'epoch': 0.02}\n",
      "{'loss': 2.2573, 'grad_norm': 2.019840717315674, 'learning_rate': 0.00029359145527369825, 'epoch': 0.02}\n",
      "{'loss': 2.1418, 'grad_norm': 1.8901102542877197, 'learning_rate': 0.0002931909212283044, 'epoch': 0.02}\n",
      "{'loss': 1.9817, 'grad_norm': 1.4123708009719849, 'learning_rate': 0.00029279038718291054, 'epoch': 0.02}\n",
      "{'loss': 2.7334, 'grad_norm': 1.6871037483215332, 'learning_rate': 0.0002923898531375167, 'epoch': 0.03}\n",
      "{'loss': 1.8652, 'grad_norm': 1.3296526670455933, 'learning_rate': 0.00029198931909212283, 'epoch': 0.03}\n",
      "{'loss': 2.1754, 'grad_norm': 1.6516492366790771, 'learning_rate': 0.0002915887850467289, 'epoch': 0.03}\n",
      "{'loss': 2.0928, 'grad_norm': 1.5061290264129639, 'learning_rate': 0.00029118825100133506, 'epoch': 0.03}\n",
      "{'loss': 2.1564, 'grad_norm': 1.351349115371704, 'learning_rate': 0.0002907877169559412, 'epoch': 0.03}\n",
      "{'loss': 2.5404, 'grad_norm': 1.4595414400100708, 'learning_rate': 0.00029038718291054735, 'epoch': 0.03}\n",
      "{'loss': 1.9141, 'grad_norm': 1.8596460819244385, 'learning_rate': 0.0002899866488651535, 'epoch': 0.03}\n",
      "{'loss': 2.1407, 'grad_norm': 2.0857458114624023, 'learning_rate': 0.00028958611481975965, 'epoch': 0.03}\n",
      "{'loss': 2.1939, 'grad_norm': 1.4737669229507446, 'learning_rate': 0.0002891855807743658, 'epoch': 0.04}\n",
      "{'loss': 1.6136, 'grad_norm': 1.9096574783325195, 'learning_rate': 0.00028878504672897194, 'epoch': 0.04}\n",
      "{'loss': 2.1613, 'grad_norm': 2.188616991043091, 'learning_rate': 0.0002883845126835781, 'epoch': 0.04}\n",
      "{'loss': 2.8946, 'grad_norm': 2.4568867683410645, 'learning_rate': 0.0002879839786381842, 'epoch': 0.04}\n",
      "{'loss': 2.4162, 'grad_norm': 1.5872048139572144, 'learning_rate': 0.00028758344459279037, 'epoch': 0.04}\n",
      "{'loss': 1.8109, 'grad_norm': 1.6606932878494263, 'learning_rate': 0.0002871829105473965, 'epoch': 0.04}\n",
      "{'loss': 1.9571, 'grad_norm': 1.6086667776107788, 'learning_rate': 0.00028678237650200266, 'epoch': 0.04}\n",
      "{'loss': 2.3367, 'grad_norm': 1.7115387916564941, 'learning_rate': 0.0002863818424566088, 'epoch': 0.05}\n",
      "{'loss': 1.8302, 'grad_norm': 1.8527249097824097, 'learning_rate': 0.0002859813084112149, 'epoch': 0.05}\n",
      "{'loss': 2.1073, 'grad_norm': 1.7276370525360107, 'learning_rate': 0.00028558077436582105, 'epoch': 0.05}\n",
      "{'loss': 1.7515, 'grad_norm': 1.934411883354187, 'learning_rate': 0.0002851802403204272, 'epoch': 0.05}\n",
      "{'loss': 1.7843, 'grad_norm': 1.4081506729125977, 'learning_rate': 0.00028477970627503334, 'epoch': 0.05}\n",
      "{'loss': 1.801, 'grad_norm': 1.275480031967163, 'learning_rate': 0.0002843791722296395, 'epoch': 0.05}\n",
      "{'loss': 2.3862, 'grad_norm': 1.3750134706497192, 'learning_rate': 0.0002839786381842456, 'epoch': 0.05}\n",
      "{'loss': 2.1266, 'grad_norm': 1.6228270530700684, 'learning_rate': 0.00028357810413885177, 'epoch': 0.05}\n",
      "{'loss': 1.5612, 'grad_norm': 2.087298631668091, 'learning_rate': 0.0002831775700934579, 'epoch': 0.06}\n",
      "{'loss': 2.1507, 'grad_norm': 1.8078386783599854, 'learning_rate': 0.00028277703604806406, 'epoch': 0.06}\n",
      "{'loss': 2.0884, 'grad_norm': 1.8627516031265259, 'learning_rate': 0.0002823765020026702, 'epoch': 0.06}\n",
      "{'loss': 2.1688, 'grad_norm': 1.7024612426757812, 'learning_rate': 0.00028197596795727635, 'epoch': 0.06}\n",
      "{'loss': 2.1227, 'grad_norm': 2.1434319019317627, 'learning_rate': 0.0002815754339118825, 'epoch': 0.06}\n",
      "{'loss': 2.4001, 'grad_norm': 1.8315659761428833, 'learning_rate': 0.00028117489986648864, 'epoch': 0.06}\n",
      "{'loss': 2.103, 'grad_norm': 1.7071729898452759, 'learning_rate': 0.0002807743658210948, 'epoch': 0.06}\n",
      "{'loss': 2.1578, 'grad_norm': 1.3928192853927612, 'learning_rate': 0.0002803738317757009, 'epoch': 0.07}\n",
      "{'loss': 2.5081, 'grad_norm': 1.8897984027862549, 'learning_rate': 0.000279973297730307, 'epoch': 0.07}\n",
      "{'loss': 1.7112, 'grad_norm': 1.5547116994857788, 'learning_rate': 0.00027957276368491317, 'epoch': 0.07}\n",
      "{'loss': 2.1913, 'grad_norm': 1.7361018657684326, 'learning_rate': 0.0002791722296395193, 'epoch': 0.07}\n",
      "{'loss': 2.383, 'grad_norm': 1.8344978094100952, 'learning_rate': 0.00027877169559412546, 'epoch': 0.07}\n",
      "{'loss': 1.831, 'grad_norm': 1.3336176872253418, 'learning_rate': 0.0002783711615487316, 'epoch': 0.07}\n",
      "{'loss': 1.6234, 'grad_norm': 1.937384843826294, 'learning_rate': 0.00027797062750333775, 'epoch': 0.07}\n",
      "{'loss': 2.2629, 'grad_norm': 1.2686394453048706, 'learning_rate': 0.0002775700934579439, 'epoch': 0.07}\n",
      "{'loss': 1.8241, 'grad_norm': 1.200632929801941, 'learning_rate': 0.00027716955941255004, 'epoch': 0.08}\n",
      "{'loss': 2.3482, 'grad_norm': 1.6991389989852905, 'learning_rate': 0.0002767690253671562, 'epoch': 0.08}\n",
      "{'loss': 2.0855, 'grad_norm': 1.7598001956939697, 'learning_rate': 0.00027636849132176234, 'epoch': 0.08}\n",
      "{'loss': 2.2702, 'grad_norm': 1.6755437850952148, 'learning_rate': 0.0002759679572763685, 'epoch': 0.08}\n",
      "{'loss': 1.6743, 'grad_norm': 2.1472854614257812, 'learning_rate': 0.0002755674232309746, 'epoch': 0.08}\n",
      "{'loss': 2.0442, 'grad_norm': 1.3513199090957642, 'learning_rate': 0.00027516688918558077, 'epoch': 0.08}\n",
      "{'loss': 1.734, 'grad_norm': 1.8619834184646606, 'learning_rate': 0.00027476635514018686, 'epoch': 0.08}\n",
      "{'loss': 2.2919, 'grad_norm': 1.463427186012268, 'learning_rate': 0.000274365821094793, 'epoch': 0.09}\n",
      "{'loss': 1.9655, 'grad_norm': 1.55111825466156, 'learning_rate': 0.00027396528704939915, 'epoch': 0.09}\n",
      "{'loss': 1.8913, 'grad_norm': 2.1017751693725586, 'learning_rate': 0.00027356475300400535, 'epoch': 0.09}\n",
      "{'loss': 2.0361, 'grad_norm': 1.3776301145553589, 'learning_rate': 0.00027316421895861144, 'epoch': 0.09}\n",
      "{'loss': 2.3787, 'grad_norm': 1.4822041988372803, 'learning_rate': 0.0002727636849132176, 'epoch': 0.09}\n",
      "{'loss': 2.2293, 'grad_norm': 2.2220160961151123, 'learning_rate': 0.00027236315086782373, 'epoch': 0.09}\n",
      "{'loss': 1.7798, 'grad_norm': 1.3955713510513306, 'learning_rate': 0.0002719626168224299, 'epoch': 0.09}\n",
      "{'loss': 1.7865, 'grad_norm': 1.758634328842163, 'learning_rate': 0.000271562082777036, 'epoch': 0.09}\n",
      "{'loss': 2.2254, 'grad_norm': 1.611173152923584, 'learning_rate': 0.00027116154873164217, 'epoch': 0.1}\n",
      "{'loss': 2.5784, 'grad_norm': 1.8856770992279053, 'learning_rate': 0.0002707610146862483, 'epoch': 0.1}\n",
      "{'loss': 1.6558, 'grad_norm': 1.6972485780715942, 'learning_rate': 0.00027036048064085446, 'epoch': 0.1}\n",
      "{'loss': 1.8717, 'grad_norm': 1.1010557413101196, 'learning_rate': 0.0002699599465954606, 'epoch': 0.1}\n",
      "{'loss': 2.3837, 'grad_norm': 1.2783329486846924, 'learning_rate': 0.00026955941255006675, 'epoch': 0.1}\n",
      "{'loss': 2.505, 'grad_norm': 2.0765762329101562, 'learning_rate': 0.00026915887850467284, 'epoch': 0.1}\n",
      "{'loss': 2.5911, 'grad_norm': 1.7384679317474365, 'learning_rate': 0.000268758344459279, 'epoch': 0.1}\n",
      "{'loss': 1.7201, 'grad_norm': 1.6008729934692383, 'learning_rate': 0.00026835781041388513, 'epoch': 0.11}\n",
      "{'loss': 1.8975, 'grad_norm': 1.318886637687683, 'learning_rate': 0.00026795727636849133, 'epoch': 0.11}\n",
      "{'loss': 2.0762, 'grad_norm': 1.2302746772766113, 'learning_rate': 0.0002675567423230974, 'epoch': 0.11}\n",
      "{'loss': 1.8752, 'grad_norm': 1.662819266319275, 'learning_rate': 0.00026715620827770357, 'epoch': 0.11}\n",
      "{'loss': 2.3602, 'grad_norm': 2.1499109268188477, 'learning_rate': 0.0002667556742323097, 'epoch': 0.11}\n",
      "{'loss': 1.906, 'grad_norm': 1.6755452156066895, 'learning_rate': 0.00026635514018691586, 'epoch': 0.11}\n",
      "{'loss': 2.3932, 'grad_norm': 1.5193156003952026, 'learning_rate': 0.000265954606141522, 'epoch': 0.11}\n",
      "{'loss': 2.2228, 'grad_norm': 1.9780915975570679, 'learning_rate': 0.00026555407209612815, 'epoch': 0.11}\n",
      "{'loss': 2.2758, 'grad_norm': 1.5874218940734863, 'learning_rate': 0.0002651535380507343, 'epoch': 0.12}\n",
      "{'loss': 1.6574, 'grad_norm': 1.648147463798523, 'learning_rate': 0.00026475300400534044, 'epoch': 0.12}\n",
      "{'loss': 1.8753, 'grad_norm': 2.0906755924224854, 'learning_rate': 0.0002643524699599466, 'epoch': 0.12}\n",
      "{'loss': 1.6423, 'grad_norm': 2.0418341159820557, 'learning_rate': 0.00026395193591455273, 'epoch': 0.12}\n",
      "{'loss': 1.8251, 'grad_norm': 1.50942862033844, 'learning_rate': 0.0002635514018691588, 'epoch': 0.12}\n",
      "{'loss': 1.9459, 'grad_norm': 1.4022458791732788, 'learning_rate': 0.00026315086782376497, 'epoch': 0.12}\n",
      "{'loss': 1.7471, 'grad_norm': 1.404489278793335, 'learning_rate': 0.0002627503337783711, 'epoch': 0.12}\n",
      "{'loss': 1.8029, 'grad_norm': 1.671539306640625, 'learning_rate': 0.0002623497997329773, 'epoch': 0.13}\n",
      "{'loss': 1.9357, 'grad_norm': 1.9534261226654053, 'learning_rate': 0.0002619492656875834, 'epoch': 0.13}\n",
      "{'loss': 1.7629, 'grad_norm': 1.704526424407959, 'learning_rate': 0.00026154873164218955, 'epoch': 0.13}\n",
      "{'loss': 1.9044, 'grad_norm': 1.8673175573349, 'learning_rate': 0.0002611481975967957, 'epoch': 0.13}\n",
      "{'loss': 2.1635, 'grad_norm': 2.4104888439178467, 'learning_rate': 0.00026074766355140184, 'epoch': 0.13}\n",
      "{'loss': 2.2832, 'grad_norm': 1.4570423364639282, 'learning_rate': 0.000260347129506008, 'epoch': 0.13}\n",
      "{'loss': 2.1175, 'grad_norm': 1.6217213869094849, 'learning_rate': 0.00025994659546061413, 'epoch': 0.13}\n",
      "{'loss': 1.9208, 'grad_norm': 1.7619595527648926, 'learning_rate': 0.0002595460614152203, 'epoch': 0.13}\n",
      "{'loss': 2.4098, 'grad_norm': 1.7344679832458496, 'learning_rate': 0.0002591455273698264, 'epoch': 0.14}\n",
      "{'loss': 2.3798, 'grad_norm': 1.5042210817337036, 'learning_rate': 0.00025874499332443257, 'epoch': 0.14}\n",
      "{'loss': 2.1008, 'grad_norm': 1.3560283184051514, 'learning_rate': 0.0002583444592790387, 'epoch': 0.14}\n",
      "{'loss': 2.0502, 'grad_norm': 1.7177842855453491, 'learning_rate': 0.0002579439252336448, 'epoch': 0.14}\n",
      "{'loss': 1.9602, 'grad_norm': 2.2131173610687256, 'learning_rate': 0.00025754339118825095, 'epoch': 0.14}\n",
      "{'loss': 1.9498, 'grad_norm': 1.1448167562484741, 'learning_rate': 0.0002571428571428571, 'epoch': 0.14}\n",
      "{'loss': 2.4663, 'grad_norm': 1.6487092971801758, 'learning_rate': 0.0002567423230974633, 'epoch': 0.14}\n",
      "{'loss': 1.7934, 'grad_norm': 1.9745107889175415, 'learning_rate': 0.0002563417890520694, 'epoch': 0.15}\n",
      "{'loss': 2.3692, 'grad_norm': 1.7173306941986084, 'learning_rate': 0.00025594125500667553, 'epoch': 0.15}\n",
      "{'loss': 2.3528, 'grad_norm': 1.4868088960647583, 'learning_rate': 0.0002555407209612817, 'epoch': 0.15}\n",
      "{'loss': 2.1432, 'grad_norm': 1.4450122117996216, 'learning_rate': 0.0002551401869158878, 'epoch': 0.15}\n",
      "{'loss': 2.0719, 'grad_norm': 1.4309561252593994, 'learning_rate': 0.00025473965287049397, 'epoch': 0.15}\n",
      "{'loss': 1.6842, 'grad_norm': 2.0405054092407227, 'learning_rate': 0.0002543391188251001, 'epoch': 0.15}\n",
      "{'loss': 2.1294, 'grad_norm': 1.593266487121582, 'learning_rate': 0.00025393858477970626, 'epoch': 0.15}\n",
      "{'loss': 2.1521, 'grad_norm': 1.328651785850525, 'learning_rate': 0.0002535380507343124, 'epoch': 0.15}\n",
      "{'loss': 2.0968, 'grad_norm': 1.7965731620788574, 'learning_rate': 0.00025313751668891855, 'epoch': 0.16}\n",
      "{'loss': 2.0363, 'grad_norm': 1.5822234153747559, 'learning_rate': 0.0002527369826435247, 'epoch': 0.16}\n",
      "{'loss': 2.2064, 'grad_norm': 2.2616705894470215, 'learning_rate': 0.0002523364485981308, 'epoch': 0.16}\n",
      "{'loss': 2.0307, 'grad_norm': 1.6317952871322632, 'learning_rate': 0.00025193591455273693, 'epoch': 0.16}\n",
      "{'loss': 2.0018, 'grad_norm': 1.7400331497192383, 'learning_rate': 0.0002515353805073431, 'epoch': 0.16}\n",
      "{'loss': 1.7426, 'grad_norm': 2.0226528644561768, 'learning_rate': 0.0002511348464619493, 'epoch': 0.16}\n",
      "{'loss': 2.5028, 'grad_norm': 1.5121479034423828, 'learning_rate': 0.00025073431241655537, 'epoch': 0.16}\n",
      "{'loss': 1.9013, 'grad_norm': 2.1445913314819336, 'learning_rate': 0.0002503337783711615, 'epoch': 0.17}\n",
      "{'loss': 2.2452, 'grad_norm': 1.6752688884735107, 'learning_rate': 0.00024993324432576766, 'epoch': 0.17}\n",
      "{'loss': 1.844, 'grad_norm': 1.9415496587753296, 'learning_rate': 0.0002495327102803738, 'epoch': 0.17}\n",
      "{'loss': 2.1081, 'grad_norm': 1.3087527751922607, 'learning_rate': 0.00024913217623497995, 'epoch': 0.17}\n",
      "{'loss': 2.7159, 'grad_norm': 1.642857551574707, 'learning_rate': 0.0002487316421895861, 'epoch': 0.17}\n",
      "{'loss': 2.1251, 'grad_norm': 1.53035569190979, 'learning_rate': 0.00024833110814419224, 'epoch': 0.17}\n",
      "{'loss': 2.171, 'grad_norm': 1.5340359210968018, 'learning_rate': 0.0002479305740987984, 'epoch': 0.17}\n",
      "{'loss': 1.7624, 'grad_norm': 1.468782901763916, 'learning_rate': 0.00024753004005340453, 'epoch': 0.17}\n",
      "{'loss': 1.9973, 'grad_norm': 1.992935299873352, 'learning_rate': 0.0002471295060080107, 'epoch': 0.18}\n",
      "{'loss': 2.2435, 'grad_norm': 1.3738963603973389, 'learning_rate': 0.00024672897196261677, 'epoch': 0.18}\n",
      "{'loss': 1.7467, 'grad_norm': 2.5208680629730225, 'learning_rate': 0.0002463284379172229, 'epoch': 0.18}\n",
      "{'loss': 2.1122, 'grad_norm': 1.615066647529602, 'learning_rate': 0.00024592790387182906, 'epoch': 0.18}\n",
      "{'loss': 2.4667, 'grad_norm': 1.5301975011825562, 'learning_rate': 0.00024552736982643526, 'epoch': 0.18}\n",
      "{'loss': 2.0129, 'grad_norm': 2.0398292541503906, 'learning_rate': 0.00024512683578104135, 'epoch': 0.18}\n",
      "{'loss': 2.2667, 'grad_norm': 1.3274718523025513, 'learning_rate': 0.0002447263017356475, 'epoch': 0.18}\n",
      "{'loss': 2.5321, 'grad_norm': 1.8042850494384766, 'learning_rate': 0.00024432576769025364, 'epoch': 0.19}\n",
      "{'loss': 2.1506, 'grad_norm': 1.5434738397598267, 'learning_rate': 0.0002439252336448598, 'epoch': 0.19}\n",
      "{'loss': 1.7426, 'grad_norm': 1.8643906116485596, 'learning_rate': 0.00024352469959946593, 'epoch': 0.19}\n",
      "{'loss': 1.632, 'grad_norm': 1.5682669878005981, 'learning_rate': 0.00024312416555407205, 'epoch': 0.19}\n",
      "{'loss': 2.0799, 'grad_norm': 1.3402105569839478, 'learning_rate': 0.00024272363150867822, 'epoch': 0.19}\n",
      "{'loss': 1.9821, 'grad_norm': 1.5655804872512817, 'learning_rate': 0.00024232309746328437, 'epoch': 0.19}\n",
      "{'loss': 1.9621, 'grad_norm': 1.4065650701522827, 'learning_rate': 0.00024192256341789051, 'epoch': 0.19}\n",
      "{'loss': 2.0498, 'grad_norm': 1.3948533535003662, 'learning_rate': 0.00024152202937249663, 'epoch': 0.19}\n",
      "{'loss': 1.9402, 'grad_norm': 1.252089500427246, 'learning_rate': 0.00024112149532710278, 'epoch': 0.2}\n",
      "{'loss': 1.7404, 'grad_norm': 2.1241567134857178, 'learning_rate': 0.00024072096128170892, 'epoch': 0.2}\n",
      "{'loss': 2.0864, 'grad_norm': 1.7765425443649292, 'learning_rate': 0.00024032042723631504, 'epoch': 0.2}\n",
      "{'loss': 2.0727, 'grad_norm': 1.872294545173645, 'learning_rate': 0.0002399198931909212, 'epoch': 0.2}\n",
      "{'loss': 1.9191, 'grad_norm': 1.6549763679504395, 'learning_rate': 0.00023951935914552736, 'epoch': 0.2}\n",
      "{'loss': 1.6818, 'grad_norm': 2.222383499145508, 'learning_rate': 0.0002391188251001335, 'epoch': 0.2}\n",
      "{'loss': 2.3687, 'grad_norm': 2.5880532264709473, 'learning_rate': 0.00023871829105473962, 'epoch': 0.2}\n",
      "{'loss': 1.8579, 'grad_norm': 2.7198283672332764, 'learning_rate': 0.00023831775700934577, 'epoch': 0.21}\n",
      "{'loss': 1.9139, 'grad_norm': 1.489042043685913, 'learning_rate': 0.0002379172229639519, 'epoch': 0.21}\n",
      "{'loss': 2.3596, 'grad_norm': 1.6306074857711792, 'learning_rate': 0.00023751668891855803, 'epoch': 0.21}\n",
      "{'loss': 1.7712, 'grad_norm': 1.6059391498565674, 'learning_rate': 0.0002371161548731642, 'epoch': 0.21}\n",
      "{'loss': 1.8845, 'grad_norm': 1.4269633293151855, 'learning_rate': 0.00023671562082777035, 'epoch': 0.21}\n",
      "{'loss': 1.731, 'grad_norm': 1.766208291053772, 'learning_rate': 0.0002363150867823765, 'epoch': 0.21}\n",
      "{'loss': 2.267, 'grad_norm': 1.2447904348373413, 'learning_rate': 0.0002359145527369826, 'epoch': 0.21}\n",
      "{'loss': 2.2437, 'grad_norm': 1.5921341180801392, 'learning_rate': 0.00023551401869158876, 'epoch': 0.21}\n",
      "{'loss': 1.8951, 'grad_norm': 1.4734776020050049, 'learning_rate': 0.0002351134846461949, 'epoch': 0.22}\n",
      "{'loss': 2.3226, 'grad_norm': 1.9398614168167114, 'learning_rate': 0.00023471295060080108, 'epoch': 0.22}\n",
      "{'loss': 1.8849, 'grad_norm': 2.0142462253570557, 'learning_rate': 0.0002343124165554072, 'epoch': 0.22}\n",
      "{'loss': 2.0734, 'grad_norm': 1.6445413827896118, 'learning_rate': 0.00023391188251001334, 'epoch': 0.22}\n",
      "{'loss': 1.7281, 'grad_norm': 1.8015307188034058, 'learning_rate': 0.00023351134846461949, 'epoch': 0.22}\n",
      "{'loss': 1.8446, 'grad_norm': 1.5240546464920044, 'learning_rate': 0.0002331108144192256, 'epoch': 0.22}\n",
      "{'loss': 2.1766, 'grad_norm': 1.898553490638733, 'learning_rate': 0.00023271028037383175, 'epoch': 0.22}\n",
      "{'loss': 2.4107, 'grad_norm': 1.5673534870147705, 'learning_rate': 0.0002323097463284379, 'epoch': 0.23}\n",
      "{'loss': 2.3473, 'grad_norm': 1.9730192422866821, 'learning_rate': 0.00023190921228304407, 'epoch': 0.23}\n",
      "{'loss': 2.3909, 'grad_norm': 1.6903096437454224, 'learning_rate': 0.00023150867823765019, 'epoch': 0.23}\n",
      "{'loss': 1.642, 'grad_norm': 1.8530112504959106, 'learning_rate': 0.00023110814419225633, 'epoch': 0.23}\n",
      "{'loss': 1.7855, 'grad_norm': 1.523680567741394, 'learning_rate': 0.00023070761014686248, 'epoch': 0.23}\n",
      "{'loss': 2.0235, 'grad_norm': 1.341983437538147, 'learning_rate': 0.0002303070761014686, 'epoch': 0.23}\n",
      "{'loss': 2.6084, 'grad_norm': 1.4977129697799683, 'learning_rate': 0.00022990654205607474, 'epoch': 0.23}\n",
      "{'loss': 1.8906, 'grad_norm': 1.7787344455718994, 'learning_rate': 0.00022950600801068089, 'epoch': 0.23}\n",
      "{'loss': 2.3231, 'grad_norm': 1.6074810028076172, 'learning_rate': 0.00022910547396528706, 'epoch': 0.24}\n",
      "{'loss': 1.8609, 'grad_norm': 1.5305224657058716, 'learning_rate': 0.00022870493991989318, 'epoch': 0.24}\n",
      "{'loss': 1.7785, 'grad_norm': 1.7471675872802734, 'learning_rate': 0.00022830440587449932, 'epoch': 0.24}\n",
      "{'loss': 2.1104, 'grad_norm': 1.676316261291504, 'learning_rate': 0.00022790387182910547, 'epoch': 0.24}\n",
      "{'loss': 2.111, 'grad_norm': 1.5630738735198975, 'learning_rate': 0.00022750333778371159, 'epoch': 0.24}\n",
      "{'loss': 2.191, 'grad_norm': 2.410390853881836, 'learning_rate': 0.00022710280373831773, 'epoch': 0.24}\n",
      "{'loss': 1.7888, 'grad_norm': 1.5610440969467163, 'learning_rate': 0.00022670226969292388, 'epoch': 0.24}\n",
      "{'loss': 2.0839, 'grad_norm': 1.230462670326233, 'learning_rate': 0.00022630173564753005, 'epoch': 0.25}\n",
      "{'loss': 2.1748, 'grad_norm': 1.3306671380996704, 'learning_rate': 0.00022590120160213617, 'epoch': 0.25}\n",
      "{'loss': 1.9972, 'grad_norm': 1.67366361618042, 'learning_rate': 0.0002255006675567423, 'epoch': 0.25}\n",
      "{'loss': 1.9893, 'grad_norm': 1.5278561115264893, 'learning_rate': 0.00022510013351134846, 'epoch': 0.25}\n",
      "{'loss': 1.641, 'grad_norm': 2.7346160411834717, 'learning_rate': 0.00022469959946595458, 'epoch': 0.25}\n",
      "{'loss': 2.377, 'grad_norm': 1.5442442893981934, 'learning_rate': 0.00022429906542056072, 'epoch': 0.25}\n",
      "{'loss': 2.0749, 'grad_norm': 1.9383025169372559, 'learning_rate': 0.00022389853137516687, 'epoch': 0.25}\n",
      "{'loss': 1.7079, 'grad_norm': 2.214906692504883, 'learning_rate': 0.00022349799732977304, 'epoch': 0.26}\n",
      "{'loss': 1.9534, 'grad_norm': 1.6359379291534424, 'learning_rate': 0.00022309746328437916, 'epoch': 0.26}\n",
      "{'loss': 2.43, 'grad_norm': 2.588148832321167, 'learning_rate': 0.0002226969292389853, 'epoch': 0.26}\n",
      "{'loss': 2.1355, 'grad_norm': 1.4489741325378418, 'learning_rate': 0.00022229639519359145, 'epoch': 0.26}\n",
      "{'loss': 2.338, 'grad_norm': 2.143472194671631, 'learning_rate': 0.00022189586114819757, 'epoch': 0.26}\n",
      "{'loss': 2.2053, 'grad_norm': 1.422068476676941, 'learning_rate': 0.0002214953271028037, 'epoch': 0.26}\n",
      "{'loss': 2.2127, 'grad_norm': 1.610732078552246, 'learning_rate': 0.00022109479305740986, 'epoch': 0.26}\n",
      "{'loss': 2.0814, 'grad_norm': 1.7293977737426758, 'learning_rate': 0.00022069425901201603, 'epoch': 0.26}\n",
      "{'loss': 1.991, 'grad_norm': 1.628393292427063, 'learning_rate': 0.00022029372496662215, 'epoch': 0.27}\n",
      "{'loss': 2.3192, 'grad_norm': 1.6800782680511475, 'learning_rate': 0.0002198931909212283, 'epoch': 0.27}\n",
      "{'loss': 1.9754, 'grad_norm': 2.1028895378112793, 'learning_rate': 0.00021949265687583444, 'epoch': 0.27}\n",
      "{'loss': 2.2923, 'grad_norm': 1.5345648527145386, 'learning_rate': 0.00021909212283044056, 'epoch': 0.27}\n",
      "{'loss': 1.9241, 'grad_norm': 3.154198408126831, 'learning_rate': 0.0002186915887850467, 'epoch': 0.27}\n",
      "{'loss': 1.8565, 'grad_norm': 1.2109607458114624, 'learning_rate': 0.00021829105473965285, 'epoch': 0.27}\n",
      "{'loss': 2.1981, 'grad_norm': 1.3878401517868042, 'learning_rate': 0.00021789052069425902, 'epoch': 0.27}\n",
      "{'loss': 1.9946, 'grad_norm': 1.7328402996063232, 'learning_rate': 0.00021748998664886514, 'epoch': 0.28}\n",
      "{'loss': 2.3596, 'grad_norm': 1.6059293746948242, 'learning_rate': 0.00021708945260347128, 'epoch': 0.28}\n",
      "{'loss': 2.0318, 'grad_norm': 1.2611948251724243, 'learning_rate': 0.00021668891855807743, 'epoch': 0.28}\n",
      "{'loss': 1.7723, 'grad_norm': 1.6966015100479126, 'learning_rate': 0.00021628838451268355, 'epoch': 0.28}\n",
      "{'loss': 1.9688, 'grad_norm': 2.814391851425171, 'learning_rate': 0.0002158878504672897, 'epoch': 0.28}\n",
      "{'loss': 2.0074, 'grad_norm': 1.5969674587249756, 'learning_rate': 0.00021548731642189584, 'epoch': 0.28}\n",
      "{'loss': 2.1027, 'grad_norm': 1.3421266078948975, 'learning_rate': 0.000215086782376502, 'epoch': 0.28}\n",
      "{'loss': 1.7443, 'grad_norm': 1.680026650428772, 'learning_rate': 0.00021468624833110813, 'epoch': 0.28}\n",
      "{'loss': 2.2946, 'grad_norm': 1.278702735900879, 'learning_rate': 0.00021428571428571427, 'epoch': 0.29}\n",
      "{'loss': 2.4435, 'grad_norm': 1.2142090797424316, 'learning_rate': 0.00021388518024032042, 'epoch': 0.29}\n",
      "{'loss': 2.111, 'grad_norm': 1.660815715789795, 'learning_rate': 0.00021348464619492654, 'epoch': 0.29}\n",
      "{'loss': 2.4195, 'grad_norm': 1.75676691532135, 'learning_rate': 0.00021308411214953268, 'epoch': 0.29}\n",
      "{'loss': 2.3659, 'grad_norm': 1.6924868822097778, 'learning_rate': 0.00021268357810413883, 'epoch': 0.29}\n",
      "{'loss': 2.3082, 'grad_norm': 1.3018684387207031, 'learning_rate': 0.000212283044058745, 'epoch': 0.29}\n",
      "{'loss': 2.0644, 'grad_norm': 1.2831367254257202, 'learning_rate': 0.00021188251001335112, 'epoch': 0.29}\n",
      "{'loss': 1.7646, 'grad_norm': 1.507336139678955, 'learning_rate': 0.00021148197596795727, 'epoch': 0.3}\n",
      "{'loss': 2.2789, 'grad_norm': 1.5388233661651611, 'learning_rate': 0.0002110814419225634, 'epoch': 0.3}\n",
      "{'loss': 2.5329, 'grad_norm': 2.440843105316162, 'learning_rate': 0.00021068090787716953, 'epoch': 0.3}\n",
      "{'loss': 1.5468, 'grad_norm': 1.9271174669265747, 'learning_rate': 0.00021028037383177567, 'epoch': 0.3}\n",
      "{'loss': 2.0492, 'grad_norm': 1.6826670169830322, 'learning_rate': 0.00020987983978638182, 'epoch': 0.3}\n",
      "{'loss': 1.8044, 'grad_norm': 1.5854066610336304, 'learning_rate': 0.000209479305740988, 'epoch': 0.3}\n",
      "{'loss': 2.4961, 'grad_norm': 2.270798683166504, 'learning_rate': 0.0002090787716955941, 'epoch': 0.3}\n",
      "{'loss': 2.1778, 'grad_norm': 1.6871217489242554, 'learning_rate': 0.00020867823765020026, 'epoch': 0.3}\n",
      "{'loss': 2.1899, 'grad_norm': 1.8583223819732666, 'learning_rate': 0.0002082777036048064, 'epoch': 0.31}\n",
      "{'loss': 1.8747, 'grad_norm': 1.5391570329666138, 'learning_rate': 0.00020787716955941252, 'epoch': 0.31}\n",
      "{'loss': 1.8181, 'grad_norm': 2.0585505962371826, 'learning_rate': 0.00020747663551401867, 'epoch': 0.31}\n",
      "{'loss': 1.4762, 'grad_norm': 2.289170742034912, 'learning_rate': 0.0002070761014686248, 'epoch': 0.31}\n",
      "{'loss': 2.0236, 'grad_norm': 1.64518141746521, 'learning_rate': 0.00020667556742323098, 'epoch': 0.31}\n",
      "{'loss': 1.8151, 'grad_norm': 1.81951105594635, 'learning_rate': 0.0002062750333778371, 'epoch': 0.31}\n",
      "{'loss': 2.2923, 'grad_norm': 1.3854951858520508, 'learning_rate': 0.00020587449933244325, 'epoch': 0.31}\n",
      "{'loss': 1.9839, 'grad_norm': 1.8265504837036133, 'learning_rate': 0.0002054739652870494, 'epoch': 0.32}\n",
      "{'loss': 1.7757, 'grad_norm': 2.682464599609375, 'learning_rate': 0.0002050734312416555, 'epoch': 0.32}\n",
      "{'loss': 2.6404, 'grad_norm': 2.138368606567383, 'learning_rate': 0.00020467289719626166, 'epoch': 0.32}\n",
      "{'loss': 1.7005, 'grad_norm': 1.8826438188552856, 'learning_rate': 0.0002042723631508678, 'epoch': 0.32}\n",
      "{'loss': 2.2546, 'grad_norm': 1.6471353769302368, 'learning_rate': 0.00020387182910547397, 'epoch': 0.32}\n",
      "{'loss': 1.736, 'grad_norm': 2.267361640930176, 'learning_rate': 0.0002034712950600801, 'epoch': 0.32}\n",
      "{'loss': 2.4638, 'grad_norm': 1.8193670511245728, 'learning_rate': 0.00020307076101468624, 'epoch': 0.32}\n",
      "{'loss': 1.6272, 'grad_norm': 1.7717788219451904, 'learning_rate': 0.00020267022696929238, 'epoch': 0.32}\n",
      "{'loss': 1.8644, 'grad_norm': 1.7745267152786255, 'learning_rate': 0.0002022696929238985, 'epoch': 0.33}\n",
      "{'loss': 1.7813, 'grad_norm': 1.863753318786621, 'learning_rate': 0.00020186915887850465, 'epoch': 0.33}\n",
      "{'loss': 2.1189, 'grad_norm': 1.313049077987671, 'learning_rate': 0.0002014686248331108, 'epoch': 0.33}\n",
      "{'loss': 1.7711, 'grad_norm': 1.8789887428283691, 'learning_rate': 0.00020106809078771696, 'epoch': 0.33}\n",
      "{'loss': 2.369, 'grad_norm': 2.0091912746429443, 'learning_rate': 0.00020066755674232308, 'epoch': 0.33}\n",
      "{'loss': 2.0321, 'grad_norm': 1.4691131114959717, 'learning_rate': 0.00020026702269692923, 'epoch': 0.33}\n",
      "{'loss': 1.9855, 'grad_norm': 1.5214735269546509, 'learning_rate': 0.00019986648865153537, 'epoch': 0.33}\n",
      "{'loss': 2.0572, 'grad_norm': 1.6540093421936035, 'learning_rate': 0.0001994659546061415, 'epoch': 0.34}\n",
      "{'loss': 1.9733, 'grad_norm': 1.2874740362167358, 'learning_rate': 0.00019906542056074764, 'epoch': 0.34}\n",
      "{'loss': 1.947, 'grad_norm': 2.029287576675415, 'learning_rate': 0.00019866488651535378, 'epoch': 0.34}\n",
      "{'loss': 2.295, 'grad_norm': 2.000602960586548, 'learning_rate': 0.00019826435246995995, 'epoch': 0.34}\n",
      "{'loss': 1.6983, 'grad_norm': 1.8994144201278687, 'learning_rate': 0.00019786381842456607, 'epoch': 0.34}\n",
      "{'loss': 1.8971, 'grad_norm': 2.1914734840393066, 'learning_rate': 0.00019746328437917222, 'epoch': 0.34}\n",
      "{'loss': 2.2686, 'grad_norm': 1.6470441818237305, 'learning_rate': 0.00019706275033377836, 'epoch': 0.34}\n",
      "{'loss': 2.2721, 'grad_norm': 1.766408085823059, 'learning_rate': 0.00019666221628838448, 'epoch': 0.34}\n",
      "{'loss': 2.3313, 'grad_norm': 1.2783091068267822, 'learning_rate': 0.00019626168224299063, 'epoch': 0.35}\n",
      "{'loss': 2.329, 'grad_norm': 1.9972569942474365, 'learning_rate': 0.00019586114819759677, 'epoch': 0.35}\n",
      "{'loss': 2.258, 'grad_norm': 1.2693431377410889, 'learning_rate': 0.00019546061415220295, 'epoch': 0.35}\n",
      "{'loss': 1.8886, 'grad_norm': 2.1699626445770264, 'learning_rate': 0.00019506008010680906, 'epoch': 0.35}\n",
      "{'loss': 2.5511, 'grad_norm': 1.6824712753295898, 'learning_rate': 0.0001946595460614152, 'epoch': 0.35}\n",
      "{'loss': 2.2001, 'grad_norm': 1.3850903511047363, 'learning_rate': 0.00019425901201602135, 'epoch': 0.35}\n",
      "{'loss': 2.1939, 'grad_norm': 1.6410274505615234, 'learning_rate': 0.00019385847797062747, 'epoch': 0.35}\n",
      "{'loss': 1.7547, 'grad_norm': 1.6245135068893433, 'learning_rate': 0.00019345794392523362, 'epoch': 0.36}\n",
      "{'loss': 2.147, 'grad_norm': 1.5084507465362549, 'learning_rate': 0.00019305740987983976, 'epoch': 0.36}\n",
      "{'loss': 2.1716, 'grad_norm': 1.574735403060913, 'learning_rate': 0.00019265687583444594, 'epoch': 0.36}\n",
      "{'loss': 2.4885, 'grad_norm': 1.4247958660125732, 'learning_rate': 0.00019225634178905205, 'epoch': 0.36}\n",
      "{'loss': 1.9785, 'grad_norm': 1.675197720527649, 'learning_rate': 0.0001918558077436582, 'epoch': 0.36}\n",
      "{'loss': 2.1971, 'grad_norm': 2.862766981124878, 'learning_rate': 0.00019145527369826435, 'epoch': 0.36}\n",
      "{'loss': 1.7247, 'grad_norm': 1.9304770231246948, 'learning_rate': 0.00019105473965287046, 'epoch': 0.36}\n",
      "{'loss': 2.5542, 'grad_norm': 1.753460168838501, 'learning_rate': 0.0001906542056074766, 'epoch': 0.36}\n",
      "{'loss': 2.4922, 'grad_norm': 1.6224931478500366, 'learning_rate': 0.00019025367156208278, 'epoch': 0.37}\n",
      "{'loss': 2.0381, 'grad_norm': 1.3469531536102295, 'learning_rate': 0.00018985313751668893, 'epoch': 0.37}\n",
      "{'loss': 1.6657, 'grad_norm': 1.8979846239089966, 'learning_rate': 0.00018945260347129505, 'epoch': 0.37}\n",
      "{'loss': 2.3095, 'grad_norm': 2.52842378616333, 'learning_rate': 0.0001890520694259012, 'epoch': 0.37}\n",
      "{'loss': 2.2788, 'grad_norm': 1.4681648015975952, 'learning_rate': 0.00018865153538050734, 'epoch': 0.37}\n",
      "{'loss': 1.6764, 'grad_norm': 2.5517969131469727, 'learning_rate': 0.00018825100133511345, 'epoch': 0.37}\n",
      "{'loss': 2.3475, 'grad_norm': 1.2804455757141113, 'learning_rate': 0.0001878504672897196, 'epoch': 0.37}\n",
      "{'loss': 1.5186, 'grad_norm': 2.0581891536712646, 'learning_rate': 0.00018744993324432577, 'epoch': 0.38}\n",
      "{'loss': 2.5011, 'grad_norm': 1.5958340167999268, 'learning_rate': 0.00018704939919893192, 'epoch': 0.38}\n",
      "{'loss': 2.1988, 'grad_norm': 2.2270915508270264, 'learning_rate': 0.00018664886515353804, 'epoch': 0.38}\n",
      "{'loss': 2.1874, 'grad_norm': 1.505027174949646, 'learning_rate': 0.00018624833110814418, 'epoch': 0.38}\n",
      "{'loss': 1.4852, 'grad_norm': 1.5012404918670654, 'learning_rate': 0.00018584779706275033, 'epoch': 0.38}\n",
      "{'loss': 1.5685, 'grad_norm': 2.049697160720825, 'learning_rate': 0.00018544726301735644, 'epoch': 0.38}\n",
      "{'loss': 2.3548, 'grad_norm': 2.6465275287628174, 'learning_rate': 0.0001850467289719626, 'epoch': 0.38}\n",
      "{'loss': 2.4229, 'grad_norm': 1.4733574390411377, 'learning_rate': 0.00018464619492656876, 'epoch': 0.38}\n",
      "{'loss': 2.1635, 'grad_norm': 1.6528596878051758, 'learning_rate': 0.0001842456608811749, 'epoch': 0.39}\n",
      "{'loss': 2.1763, 'grad_norm': 1.3612643480300903, 'learning_rate': 0.00018384512683578103, 'epoch': 0.39}\n",
      "{'loss': 1.9975, 'grad_norm': 2.028125762939453, 'learning_rate': 0.00018344459279038717, 'epoch': 0.39}\n",
      "{'loss': 2.2492, 'grad_norm': 1.5399715900421143, 'learning_rate': 0.00018304405874499332, 'epoch': 0.39}\n",
      "{'loss': 2.1368, 'grad_norm': 1.4446256160736084, 'learning_rate': 0.00018264352469959944, 'epoch': 0.39}\n",
      "{'loss': 2.4449, 'grad_norm': 1.8672541379928589, 'learning_rate': 0.00018224299065420558, 'epoch': 0.39}\n",
      "{'loss': 1.7164, 'grad_norm': 1.6712613105773926, 'learning_rate': 0.00018184245660881175, 'epoch': 0.39}\n",
      "{'loss': 2.6723, 'grad_norm': 1.6935033798217773, 'learning_rate': 0.0001814419225634179, 'epoch': 0.4}\n",
      "{'loss': 2.373, 'grad_norm': 1.7204736471176147, 'learning_rate': 0.00018104138851802402, 'epoch': 0.4}\n",
      "{'loss': 2.414, 'grad_norm': 1.7854069471359253, 'learning_rate': 0.00018064085447263016, 'epoch': 0.4}\n",
      "{'loss': 2.4934, 'grad_norm': 1.9548991918563843, 'learning_rate': 0.0001802403204272363, 'epoch': 0.4}\n",
      "{'loss': 2.3288, 'grad_norm': 1.668860912322998, 'learning_rate': 0.00017983978638184243, 'epoch': 0.4}\n",
      "{'loss': 1.6655, 'grad_norm': 1.7157166004180908, 'learning_rate': 0.00017943925233644857, 'epoch': 0.4}\n",
      "{'loss': 1.8975, 'grad_norm': 2.2362897396087646, 'learning_rate': 0.00017903871829105474, 'epoch': 0.4}\n",
      "{'loss': 2.506, 'grad_norm': 1.428939700126648, 'learning_rate': 0.00017863818424566086, 'epoch': 0.4}\n",
      "{'loss': 2.569, 'grad_norm': 1.9132519960403442, 'learning_rate': 0.000178237650200267, 'epoch': 0.41}\n",
      "{'loss': 2.773, 'grad_norm': 2.0072836875915527, 'learning_rate': 0.00017783711615487315, 'epoch': 0.41}\n",
      "{'loss': 2.0913, 'grad_norm': 2.0173511505126953, 'learning_rate': 0.0001774365821094793, 'epoch': 0.41}\n",
      "{'loss': 2.2576, 'grad_norm': 1.637170672416687, 'learning_rate': 0.00017703604806408542, 'epoch': 0.41}\n",
      "{'loss': 1.9218, 'grad_norm': 1.9776581525802612, 'learning_rate': 0.00017663551401869156, 'epoch': 0.41}\n",
      "{'loss': 2.0958, 'grad_norm': 2.141608238220215, 'learning_rate': 0.00017623497997329773, 'epoch': 0.41}\n",
      "{'loss': 2.0876, 'grad_norm': 1.0587217807769775, 'learning_rate': 0.00017583444592790385, 'epoch': 0.41}\n",
      "{'loss': 2.5365, 'grad_norm': 1.8266851902008057, 'learning_rate': 0.00017543391188251, 'epoch': 0.42}\n",
      "{'loss': 2.4433, 'grad_norm': 1.295168161392212, 'learning_rate': 0.00017503337783711614, 'epoch': 0.42}\n",
      "{'loss': 2.242, 'grad_norm': 1.695780634880066, 'learning_rate': 0.0001746328437917223, 'epoch': 0.42}\n",
      "{'loss': 2.217, 'grad_norm': 1.7634556293487549, 'learning_rate': 0.0001742323097463284, 'epoch': 0.42}\n",
      "{'loss': 2.5799, 'grad_norm': 1.7636853456497192, 'learning_rate': 0.00017383177570093455, 'epoch': 0.42}\n",
      "{'loss': 2.3669, 'grad_norm': 2.452831268310547, 'learning_rate': 0.00017343124165554073, 'epoch': 0.42}\n",
      "{'loss': 2.2908, 'grad_norm': 1.634798526763916, 'learning_rate': 0.00017303070761014684, 'epoch': 0.42}\n",
      "{'loss': 1.8671, 'grad_norm': 1.5434435606002808, 'learning_rate': 0.000172630173564753, 'epoch': 0.42}\n",
      "{'loss': 2.601, 'grad_norm': 1.3627856969833374, 'learning_rate': 0.00017222963951935913, 'epoch': 0.43}\n",
      "{'loss': 1.7809, 'grad_norm': 1.835292935371399, 'learning_rate': 0.00017182910547396528, 'epoch': 0.43}\n",
      "{'loss': 2.0861, 'grad_norm': 2.1893978118896484, 'learning_rate': 0.0001714285714285714, 'epoch': 0.43}\n",
      "{'loss': 1.7452, 'grad_norm': 1.238649606704712, 'learning_rate': 0.00017102803738317754, 'epoch': 0.43}\n",
      "{'loss': 2.4071, 'grad_norm': 1.8547714948654175, 'learning_rate': 0.00017062750333778372, 'epoch': 0.43}\n",
      "{'loss': 1.8274, 'grad_norm': 2.17120361328125, 'learning_rate': 0.00017022696929238983, 'epoch': 0.43}\n",
      "{'loss': 1.6317, 'grad_norm': 2.132767677307129, 'learning_rate': 0.00016982643524699598, 'epoch': 0.43}\n",
      "{'loss': 2.5486, 'grad_norm': 2.1887717247009277, 'learning_rate': 0.00016942590120160213, 'epoch': 0.44}\n",
      "{'loss': 2.1994, 'grad_norm': 2.0873208045959473, 'learning_rate': 0.00016902536715620827, 'epoch': 0.44}\n",
      "{'loss': 2.2077, 'grad_norm': 2.199209213256836, 'learning_rate': 0.0001686248331108144, 'epoch': 0.44}\n",
      "{'loss': 2.1589, 'grad_norm': 1.5213966369628906, 'learning_rate': 0.00016822429906542053, 'epoch': 0.44}\n",
      "{'loss': 1.7918, 'grad_norm': 1.5665637254714966, 'learning_rate': 0.0001678237650200267, 'epoch': 0.44}\n",
      "{'loss': 2.3441, 'grad_norm': 1.9318004846572876, 'learning_rate': 0.00016742323097463282, 'epoch': 0.44}\n",
      "{'loss': 2.0553, 'grad_norm': 1.7609305381774902, 'learning_rate': 0.00016702269692923897, 'epoch': 0.44}\n",
      "{'loss': 2.0166, 'grad_norm': 1.4143537282943726, 'learning_rate': 0.00016662216288384512, 'epoch': 0.44}\n",
      "{'loss': 1.8187, 'grad_norm': 1.3197941780090332, 'learning_rate': 0.00016622162883845126, 'epoch': 0.45}\n",
      "{'loss': 1.8726, 'grad_norm': 1.6394087076187134, 'learning_rate': 0.00016582109479305738, 'epoch': 0.45}\n",
      "{'loss': 2.1989, 'grad_norm': 1.5739890336990356, 'learning_rate': 0.00016542056074766352, 'epoch': 0.45}\n",
      "{'loss': 2.4503, 'grad_norm': 1.3532296419143677, 'learning_rate': 0.0001650200267022697, 'epoch': 0.45}\n",
      "{'loss': 2.2397, 'grad_norm': 2.3338234424591064, 'learning_rate': 0.00016461949265687582, 'epoch': 0.45}\n",
      "{'loss': 2.0789, 'grad_norm': 2.1427955627441406, 'learning_rate': 0.00016421895861148196, 'epoch': 0.45}\n",
      "{'loss': 2.6122, 'grad_norm': 2.185249090194702, 'learning_rate': 0.0001638184245660881, 'epoch': 0.45}\n",
      "{'loss': 2.1737, 'grad_norm': 1.44374680519104, 'learning_rate': 0.00016341789052069425, 'epoch': 0.46}\n",
      "{'loss': 2.5078, 'grad_norm': 2.434587001800537, 'learning_rate': 0.00016301735647530037, 'epoch': 0.46}\n",
      "{'loss': 2.2393, 'grad_norm': 1.476724624633789, 'learning_rate': 0.00016261682242990652, 'epoch': 0.46}\n",
      "{'loss': 2.0887, 'grad_norm': 1.4624155759811401, 'learning_rate': 0.0001622162883845127, 'epoch': 0.46}\n",
      "{'loss': 2.1832, 'grad_norm': 1.4132083654403687, 'learning_rate': 0.0001618157543391188, 'epoch': 0.46}\n",
      "{'loss': 1.5699, 'grad_norm': 1.8013039827346802, 'learning_rate': 0.00016141522029372495, 'epoch': 0.46}\n",
      "{'loss': 2.5729, 'grad_norm': 1.6150753498077393, 'learning_rate': 0.0001610146862483311, 'epoch': 0.46}\n",
      "{'loss': 2.0478, 'grad_norm': 1.450944185256958, 'learning_rate': 0.00016061415220293724, 'epoch': 0.46}\n",
      "{'loss': 2.4569, 'grad_norm': 1.3663127422332764, 'learning_rate': 0.00016021361815754336, 'epoch': 0.47}\n",
      "{'loss': 2.3856, 'grad_norm': 1.507033348083496, 'learning_rate': 0.0001598130841121495, 'epoch': 0.47}\n",
      "{'loss': 2.2268, 'grad_norm': 1.317579984664917, 'learning_rate': 0.00015941255006675568, 'epoch': 0.47}\n",
      "{'loss': 2.3608, 'grad_norm': 1.1560771465301514, 'learning_rate': 0.0001590120160213618, 'epoch': 0.47}\n",
      "{'loss': 2.2937, 'grad_norm': 1.4854570627212524, 'learning_rate': 0.00015861148197596794, 'epoch': 0.47}\n",
      "{'loss': 2.4733, 'grad_norm': 1.4752804040908813, 'learning_rate': 0.0001582109479305741, 'epoch': 0.47}\n",
      "{'loss': 2.0648, 'grad_norm': 1.413548231124878, 'learning_rate': 0.00015781041388518023, 'epoch': 0.47}\n",
      "{'loss': 2.4719, 'grad_norm': 1.2579624652862549, 'learning_rate': 0.00015740987983978635, 'epoch': 0.48}\n",
      "{'loss': 2.4414, 'grad_norm': 1.451659917831421, 'learning_rate': 0.0001570093457943925, 'epoch': 0.48}\n",
      "{'loss': 2.3314, 'grad_norm': 1.7938647270202637, 'learning_rate': 0.00015660881174899867, 'epoch': 0.48}\n",
      "{'loss': 1.6332, 'grad_norm': 2.5047779083251953, 'learning_rate': 0.0001562082777036048, 'epoch': 0.48}\n",
      "{'loss': 2.5504, 'grad_norm': 1.8751718997955322, 'learning_rate': 0.00015580774365821093, 'epoch': 0.48}\n",
      "{'loss': 2.1033, 'grad_norm': 1.7326542139053345, 'learning_rate': 0.00015540720961281708, 'epoch': 0.48}\n",
      "{'loss': 1.9992, 'grad_norm': 1.7373449802398682, 'learning_rate': 0.00015500667556742322, 'epoch': 0.48}\n",
      "{'loss': 2.167, 'grad_norm': 1.4281253814697266, 'learning_rate': 0.00015460614152202934, 'epoch': 0.48}\n",
      "{'loss': 2.1639, 'grad_norm': 1.2700276374816895, 'learning_rate': 0.0001542056074766355, 'epoch': 0.49}\n",
      "{'loss': 1.8196, 'grad_norm': 1.9049534797668457, 'learning_rate': 0.00015380507343124166, 'epoch': 0.49}\n",
      "{'loss': 1.8983, 'grad_norm': 1.384678602218628, 'learning_rate': 0.00015340453938584778, 'epoch': 0.49}\n",
      "{'loss': 1.8109, 'grad_norm': 1.6511074304580688, 'learning_rate': 0.00015300400534045392, 'epoch': 0.49}\n",
      "{'loss': 2.1284, 'grad_norm': 1.2662068605422974, 'learning_rate': 0.00015260347129506007, 'epoch': 0.49}\n",
      "{'loss': 2.1016, 'grad_norm': 1.397208571434021, 'learning_rate': 0.00015220293724966621, 'epoch': 0.49}\n",
      "{'loss': 1.6402, 'grad_norm': 2.862779378890991, 'learning_rate': 0.00015180240320427233, 'epoch': 0.49}\n",
      "{'loss': 2.082, 'grad_norm': 1.360862135887146, 'learning_rate': 0.00015140186915887848, 'epoch': 0.5}\n",
      "{'loss': 1.8974, 'grad_norm': 1.4346771240234375, 'learning_rate': 0.00015100133511348465, 'epoch': 0.5}\n",
      "{'loss': 2.0785, 'grad_norm': 1.248435139656067, 'learning_rate': 0.00015060080106809077, 'epoch': 0.5}\n",
      "{'loss': 2.5553, 'grad_norm': 1.796509027481079, 'learning_rate': 0.00015020026702269691, 'epoch': 0.5}\n",
      "{'loss': 1.6281, 'grad_norm': 1.9068560600280762, 'learning_rate': 0.00014979973297730306, 'epoch': 0.5}\n",
      "{'loss': 2.0205, 'grad_norm': 1.5176985263824463, 'learning_rate': 0.0001493991989319092, 'epoch': 0.5}\n",
      "{'loss': 1.8761, 'grad_norm': 1.5385072231292725, 'learning_rate': 0.00014899866488651535, 'epoch': 0.5}\n",
      "{'loss': 1.886, 'grad_norm': 2.563241720199585, 'learning_rate': 0.00014859813084112147, 'epoch': 0.5}\n",
      "{'loss': 1.9372, 'grad_norm': 2.106879949569702, 'learning_rate': 0.00014819759679572761, 'epoch': 0.51}\n",
      "{'loss': 1.8654, 'grad_norm': 2.0213191509246826, 'learning_rate': 0.00014779706275033376, 'epoch': 0.51}\n",
      "{'loss': 1.4834, 'grad_norm': 1.939687967300415, 'learning_rate': 0.0001473965287049399, 'epoch': 0.51}\n",
      "{'loss': 2.5719, 'grad_norm': 2.2447268962860107, 'learning_rate': 0.00014699599465954605, 'epoch': 0.51}\n",
      "{'loss': 2.0275, 'grad_norm': 1.9380443096160889, 'learning_rate': 0.0001465954606141522, 'epoch': 0.51}\n",
      "{'loss': 1.8829, 'grad_norm': 2.5111827850341797, 'learning_rate': 0.00014619492656875834, 'epoch': 0.51}\n",
      "{'loss': 1.7503, 'grad_norm': 1.6511037349700928, 'learning_rate': 0.00014579439252336446, 'epoch': 0.51}\n",
      "{'loss': 1.8774, 'grad_norm': 2.326209306716919, 'learning_rate': 0.0001453938584779706, 'epoch': 0.52}\n",
      "{'loss': 1.7332, 'grad_norm': 1.6903252601623535, 'learning_rate': 0.00014499332443257675, 'epoch': 0.52}\n",
      "{'loss': 1.8556, 'grad_norm': 2.4948878288269043, 'learning_rate': 0.0001445927903871829, 'epoch': 0.52}\n",
      "{'loss': 2.3302, 'grad_norm': 2.007147789001465, 'learning_rate': 0.00014419225634178904, 'epoch': 0.52}\n",
      "{'loss': 2.3983, 'grad_norm': 1.4226558208465576, 'learning_rate': 0.00014379172229639519, 'epoch': 0.52}\n",
      "{'loss': 1.7737, 'grad_norm': 1.7158256769180298, 'learning_rate': 0.00014339118825100133, 'epoch': 0.52}\n",
      "{'loss': 1.9829, 'grad_norm': 1.9434444904327393, 'learning_rate': 0.00014299065420560745, 'epoch': 0.52}\n",
      "{'loss': 2.1677, 'grad_norm': 1.47372567653656, 'learning_rate': 0.0001425901201602136, 'epoch': 0.52}\n",
      "{'loss': 2.0963, 'grad_norm': 1.435196042060852, 'learning_rate': 0.00014218958611481974, 'epoch': 0.53}\n",
      "{'loss': 1.7675, 'grad_norm': 2.063023567199707, 'learning_rate': 0.00014178905206942589, 'epoch': 0.53}\n",
      "{'loss': 1.8898, 'grad_norm': 2.0948288440704346, 'learning_rate': 0.00014138851802403203, 'epoch': 0.53}\n",
      "{'loss': 2.1224, 'grad_norm': 1.3882957696914673, 'learning_rate': 0.00014098798397863818, 'epoch': 0.53}\n",
      "{'loss': 2.2363, 'grad_norm': 2.556922435760498, 'learning_rate': 0.00014058744993324432, 'epoch': 0.53}\n",
      "{'loss': 1.967, 'grad_norm': 1.388096809387207, 'learning_rate': 0.00014018691588785044, 'epoch': 0.53}\n",
      "{'loss': 1.6489, 'grad_norm': 1.6191262006759644, 'learning_rate': 0.00013978638184245659, 'epoch': 0.53}\n",
      "{'loss': 1.6184, 'grad_norm': 1.7455596923828125, 'learning_rate': 0.00013938584779706273, 'epoch': 0.54}\n",
      "{'loss': 1.6358, 'grad_norm': 1.5295066833496094, 'learning_rate': 0.00013898531375166888, 'epoch': 0.54}\n",
      "{'loss': 1.8955, 'grad_norm': 2.4000535011291504, 'learning_rate': 0.00013858477970627502, 'epoch': 0.54}\n",
      "{'loss': 1.9211, 'grad_norm': 2.069803237915039, 'learning_rate': 0.00013818424566088117, 'epoch': 0.54}\n",
      "{'loss': 2.1215, 'grad_norm': 1.744981050491333, 'learning_rate': 0.0001377837116154873, 'epoch': 0.54}\n",
      "{'loss': 1.5888, 'grad_norm': 1.9519068002700806, 'learning_rate': 0.00013738317757009343, 'epoch': 0.54}\n",
      "{'loss': 2.5367, 'grad_norm': 2.203120708465576, 'learning_rate': 0.00013698264352469958, 'epoch': 0.54}\n",
      "{'loss': 2.1062, 'grad_norm': 1.6612067222595215, 'learning_rate': 0.00013658210947930572, 'epoch': 0.54}\n",
      "{'loss': 2.4516, 'grad_norm': 1.4763121604919434, 'learning_rate': 0.00013618157543391187, 'epoch': 0.55}\n",
      "{'loss': 1.6835, 'grad_norm': 1.7599761486053467, 'learning_rate': 0.000135781041388518, 'epoch': 0.55}\n",
      "{'loss': 2.329, 'grad_norm': 2.251063585281372, 'learning_rate': 0.00013538050734312416, 'epoch': 0.55}\n",
      "{'loss': 1.875, 'grad_norm': 1.627698540687561, 'learning_rate': 0.0001349799732977303, 'epoch': 0.55}\n",
      "{'loss': 2.023, 'grad_norm': 1.7565901279449463, 'learning_rate': 0.00013457943925233642, 'epoch': 0.55}\n",
      "{'loss': 1.6723, 'grad_norm': 1.7760138511657715, 'learning_rate': 0.00013417890520694257, 'epoch': 0.55}\n",
      "{'loss': 1.7168, 'grad_norm': 1.6848416328430176, 'learning_rate': 0.0001337783711615487, 'epoch': 0.55}\n",
      "{'loss': 2.8297, 'grad_norm': 2.165205240249634, 'learning_rate': 0.00013337783711615486, 'epoch': 0.56}\n",
      "{'loss': 2.176, 'grad_norm': 2.3136863708496094, 'learning_rate': 0.000132977303070761, 'epoch': 0.56}\n",
      "{'loss': 2.3214, 'grad_norm': 1.7504535913467407, 'learning_rate': 0.00013257676902536715, 'epoch': 0.56}\n",
      "{'loss': 1.8856, 'grad_norm': 1.9511529207229614, 'learning_rate': 0.0001321762349799733, 'epoch': 0.56}\n",
      "{'loss': 2.1305, 'grad_norm': 1.4491242170333862, 'learning_rate': 0.0001317757009345794, 'epoch': 0.56}\n",
      "{'loss': 2.4009, 'grad_norm': 1.798913836479187, 'learning_rate': 0.00013137516688918556, 'epoch': 0.56}\n",
      "{'loss': 1.8627, 'grad_norm': 1.76009202003479, 'learning_rate': 0.0001309746328437917, 'epoch': 0.56}\n",
      "{'loss': 1.6272, 'grad_norm': 1.7265574932098389, 'learning_rate': 0.00013057409879839785, 'epoch': 0.56}\n",
      "{'loss': 1.9146, 'grad_norm': 1.9986604452133179, 'learning_rate': 0.000130173564753004, 'epoch': 0.57}\n",
      "{'loss': 2.105, 'grad_norm': 1.9780713319778442, 'learning_rate': 0.00012977303070761014, 'epoch': 0.57}\n",
      "{'loss': 1.5439, 'grad_norm': 2.1553378105163574, 'learning_rate': 0.00012937249666221628, 'epoch': 0.57}\n",
      "{'loss': 2.2155, 'grad_norm': 2.0435352325439453, 'learning_rate': 0.0001289719626168224, 'epoch': 0.57}\n",
      "{'loss': 2.0168, 'grad_norm': 1.8210177421569824, 'learning_rate': 0.00012857142857142855, 'epoch': 0.57}\n",
      "{'loss': 2.139, 'grad_norm': 1.9610240459442139, 'learning_rate': 0.0001281708945260347, 'epoch': 0.57}\n",
      "{'loss': 1.7392, 'grad_norm': 1.5490065813064575, 'learning_rate': 0.00012777036048064084, 'epoch': 0.57}\n",
      "{'loss': 1.5866, 'grad_norm': 1.7743535041809082, 'learning_rate': 0.00012736982643524698, 'epoch': 0.58}\n",
      "{'loss': 2.435, 'grad_norm': 1.3823269605636597, 'learning_rate': 0.00012696929238985313, 'epoch': 0.58}\n",
      "{'loss': 2.0799, 'grad_norm': 1.6917709112167358, 'learning_rate': 0.00012656875834445928, 'epoch': 0.58}\n",
      "{'loss': 2.1919, 'grad_norm': 1.9576061964035034, 'learning_rate': 0.0001261682242990654, 'epoch': 0.58}\n",
      "{'loss': 2.4799, 'grad_norm': 1.8960729837417603, 'learning_rate': 0.00012576769025367154, 'epoch': 0.58}\n",
      "{'loss': 1.7221, 'grad_norm': 1.417295217514038, 'learning_rate': 0.00012536715620827768, 'epoch': 0.58}\n",
      "{'loss': 2.9191, 'grad_norm': 1.9744539260864258, 'learning_rate': 0.00012496662216288383, 'epoch': 0.58}\n",
      "{'loss': 2.3717, 'grad_norm': 1.5878413915634155, 'learning_rate': 0.00012456608811748998, 'epoch': 0.58}\n",
      "{'loss': 2.4909, 'grad_norm': 1.6229612827301025, 'learning_rate': 0.00012416555407209612, 'epoch': 0.59}\n",
      "{'loss': 2.562, 'grad_norm': 2.4607558250427246, 'learning_rate': 0.00012376502002670227, 'epoch': 0.59}\n",
      "{'loss': 2.3487, 'grad_norm': 1.5476216077804565, 'learning_rate': 0.00012336448598130838, 'epoch': 0.59}\n",
      "{'loss': 1.7625, 'grad_norm': 1.4349647760391235, 'learning_rate': 0.00012296395193591453, 'epoch': 0.59}\n",
      "{'loss': 1.6534, 'grad_norm': 2.69681453704834, 'learning_rate': 0.00012256341789052068, 'epoch': 0.59}\n",
      "{'loss': 2.0054, 'grad_norm': 1.9351634979248047, 'learning_rate': 0.00012216288384512682, 'epoch': 0.59}\n",
      "{'loss': 1.9599, 'grad_norm': 1.4874087572097778, 'learning_rate': 0.00012176234979973297, 'epoch': 0.59}\n",
      "{'loss': 2.4346, 'grad_norm': 1.758133053779602, 'learning_rate': 0.00012136181575433911, 'epoch': 0.6}\n",
      "{'loss': 1.9439, 'grad_norm': 1.5229923725128174, 'learning_rate': 0.00012096128170894526, 'epoch': 0.6}\n",
      "{'loss': 2.2595, 'grad_norm': 1.2901630401611328, 'learning_rate': 0.00012056074766355139, 'epoch': 0.6}\n",
      "{'loss': 2.5364, 'grad_norm': 1.1540101766586304, 'learning_rate': 0.00012016021361815752, 'epoch': 0.6}\n",
      "{'loss': 2.0435, 'grad_norm': 1.7561619281768799, 'learning_rate': 0.00011975967957276368, 'epoch': 0.6}\n",
      "{'loss': 2.3767, 'grad_norm': 1.5318715572357178, 'learning_rate': 0.00011935914552736981, 'epoch': 0.6}\n",
      "{'loss': 2.0879, 'grad_norm': 1.6969294548034668, 'learning_rate': 0.00011895861148197596, 'epoch': 0.6}\n",
      "{'loss': 2.1039, 'grad_norm': 1.8796472549438477, 'learning_rate': 0.0001185580774365821, 'epoch': 0.6}\n",
      "{'loss': 2.0765, 'grad_norm': 1.443952202796936, 'learning_rate': 0.00011815754339118825, 'epoch': 0.61}\n",
      "{'loss': 2.3721, 'grad_norm': 2.0079774856567383, 'learning_rate': 0.00011775700934579438, 'epoch': 0.61}\n",
      "{'loss': 2.1488, 'grad_norm': 1.5450654029846191, 'learning_rate': 0.00011735647530040054, 'epoch': 0.61}\n",
      "{'loss': 2.0499, 'grad_norm': 1.8818624019622803, 'learning_rate': 0.00011695594125500667, 'epoch': 0.61}\n",
      "{'loss': 2.427, 'grad_norm': 2.123769760131836, 'learning_rate': 0.0001165554072096128, 'epoch': 0.61}\n",
      "{'loss': 2.4105, 'grad_norm': 1.6900620460510254, 'learning_rate': 0.00011615487316421895, 'epoch': 0.61}\n",
      "{'loss': 1.8542, 'grad_norm': 2.335989475250244, 'learning_rate': 0.00011575433911882509, 'epoch': 0.61}\n",
      "{'loss': 2.4882, 'grad_norm': 1.659397840499878, 'learning_rate': 0.00011535380507343124, 'epoch': 0.62}\n",
      "{'loss': 2.1097, 'grad_norm': 1.9848198890686035, 'learning_rate': 0.00011495327102803737, 'epoch': 0.62}\n",
      "{'loss': 1.9061, 'grad_norm': 1.8436752557754517, 'learning_rate': 0.00011455273698264353, 'epoch': 0.62}\n",
      "{'loss': 2.5253, 'grad_norm': 1.4382514953613281, 'learning_rate': 0.00011415220293724966, 'epoch': 0.62}\n",
      "{'loss': 1.4514, 'grad_norm': 3.021364688873291, 'learning_rate': 0.00011375166889185579, 'epoch': 0.62}\n",
      "{'loss': 1.8403, 'grad_norm': 1.6323190927505493, 'learning_rate': 0.00011335113484646194, 'epoch': 0.62}\n",
      "{'loss': 1.7647, 'grad_norm': 1.777867317199707, 'learning_rate': 0.00011295060080106808, 'epoch': 0.62}\n",
      "{'loss': 2.1824, 'grad_norm': 1.5598937273025513, 'learning_rate': 0.00011255006675567423, 'epoch': 0.62}\n",
      "{'loss': 2.2622, 'grad_norm': 1.1561695337295532, 'learning_rate': 0.00011214953271028036, 'epoch': 0.63}\n",
      "{'loss': 1.9122, 'grad_norm': 1.2427973747253418, 'learning_rate': 0.00011174899866488652, 'epoch': 0.63}\n",
      "{'loss': 2.2095, 'grad_norm': 1.8987692594528198, 'learning_rate': 0.00011134846461949265, 'epoch': 0.63}\n",
      "{'loss': 2.4068, 'grad_norm': 1.1851658821105957, 'learning_rate': 0.00011094793057409878, 'epoch': 0.63}\n",
      "{'loss': 2.0081, 'grad_norm': 1.2351676225662231, 'learning_rate': 0.00011054739652870493, 'epoch': 0.63}\n",
      "{'loss': 2.6417, 'grad_norm': 1.3636001348495483, 'learning_rate': 0.00011014686248331107, 'epoch': 0.63}\n",
      "{'loss': 2.4448, 'grad_norm': 1.0811690092086792, 'learning_rate': 0.00010974632843791722, 'epoch': 0.63}\n",
      "{'loss': 2.2703, 'grad_norm': 1.6535052061080933, 'learning_rate': 0.00010934579439252335, 'epoch': 0.64}\n",
      "{'loss': 2.4493, 'grad_norm': 1.480396032333374, 'learning_rate': 0.00010894526034712951, 'epoch': 0.64}\n",
      "{'loss': 2.6367, 'grad_norm': 1.6428265571594238, 'learning_rate': 0.00010854472630173564, 'epoch': 0.64}\n",
      "{'loss': 1.9314, 'grad_norm': 1.7378013134002686, 'learning_rate': 0.00010814419225634177, 'epoch': 0.64}\n",
      "{'loss': 2.8146, 'grad_norm': 1.257929801940918, 'learning_rate': 0.00010774365821094792, 'epoch': 0.64}\n",
      "{'loss': 2.1668, 'grad_norm': 1.303301453590393, 'learning_rate': 0.00010734312416555406, 'epoch': 0.64}\n",
      "{'loss': 2.0654, 'grad_norm': 1.4402464628219604, 'learning_rate': 0.00010694259012016021, 'epoch': 0.64}\n",
      "{'loss': 2.3497, 'grad_norm': 1.671427845954895, 'learning_rate': 0.00010654205607476634, 'epoch': 0.64}\n",
      "{'loss': 2.0032, 'grad_norm': 1.225208044052124, 'learning_rate': 0.0001061415220293725, 'epoch': 0.65}\n",
      "{'loss': 1.2686, 'grad_norm': 2.3544323444366455, 'learning_rate': 0.00010574098798397863, 'epoch': 0.65}\n",
      "{'loss': 1.691, 'grad_norm': 1.5119664669036865, 'learning_rate': 0.00010534045393858476, 'epoch': 0.65}\n",
      "{'loss': 2.2845, 'grad_norm': 1.58262038230896, 'learning_rate': 0.00010493991989319091, 'epoch': 0.65}\n",
      "{'loss': 1.9927, 'grad_norm': 1.6244102716445923, 'learning_rate': 0.00010453938584779706, 'epoch': 0.65}\n",
      "{'loss': 2.4624, 'grad_norm': 1.2530399560928345, 'learning_rate': 0.0001041388518024032, 'epoch': 0.65}\n",
      "{'loss': 2.6919, 'grad_norm': 1.5843790769577026, 'learning_rate': 0.00010373831775700933, 'epoch': 0.65}\n",
      "{'loss': 1.7235, 'grad_norm': 1.7022976875305176, 'learning_rate': 0.00010333778371161549, 'epoch': 0.66}\n",
      "{'loss': 2.2421, 'grad_norm': 1.568110466003418, 'learning_rate': 0.00010293724966622162, 'epoch': 0.66}\n",
      "{'loss': 2.1589, 'grad_norm': 1.391311764717102, 'learning_rate': 0.00010253671562082776, 'epoch': 0.66}\n",
      "{'loss': 2.2139, 'grad_norm': 1.3852156400680542, 'learning_rate': 0.0001021361815754339, 'epoch': 0.66}\n",
      "{'loss': 2.6723, 'grad_norm': 1.6280198097229004, 'learning_rate': 0.00010173564753004005, 'epoch': 0.66}\n",
      "{'loss': 2.2824, 'grad_norm': 1.5330451726913452, 'learning_rate': 0.00010133511348464619, 'epoch': 0.66}\n",
      "{'loss': 1.5552, 'grad_norm': 1.7243866920471191, 'learning_rate': 0.00010093457943925232, 'epoch': 0.66}\n",
      "{'loss': 2.6958, 'grad_norm': 1.5304299592971802, 'learning_rate': 0.00010053404539385848, 'epoch': 0.66}\n",
      "{'loss': 2.7392, 'grad_norm': 2.3612496852874756, 'learning_rate': 0.00010013351134846461, 'epoch': 0.67}\n",
      "{'loss': 1.8258, 'grad_norm': 1.5986707210540771, 'learning_rate': 9.973297730307075e-05, 'epoch': 0.67}\n",
      "{'loss': 1.9814, 'grad_norm': 1.495741367340088, 'learning_rate': 9.933244325767689e-05, 'epoch': 0.67}\n",
      "{'loss': 2.1243, 'grad_norm': 2.2606024742126465, 'learning_rate': 9.893190921228304e-05, 'epoch': 0.67}\n",
      "{'loss': 1.6405, 'grad_norm': 1.9725247621536255, 'learning_rate': 9.853137516688918e-05, 'epoch': 0.67}\n",
      "{'loss': 1.9737, 'grad_norm': 1.8825522661209106, 'learning_rate': 9.813084112149531e-05, 'epoch': 0.67}\n",
      "{'loss': 2.2167, 'grad_norm': 1.4401756525039673, 'learning_rate': 9.773030707610147e-05, 'epoch': 0.67}\n",
      "{'loss': 2.1817, 'grad_norm': 1.5418596267700195, 'learning_rate': 9.73297730307076e-05, 'epoch': 0.68}\n",
      "{'loss': 1.696, 'grad_norm': 1.5406463146209717, 'learning_rate': 9.692923898531374e-05, 'epoch': 0.68}\n",
      "{'loss': 2.3223, 'grad_norm': 1.3020068407058716, 'learning_rate': 9.652870493991988e-05, 'epoch': 0.68}\n",
      "{'loss': 1.7406, 'grad_norm': 2.618112087249756, 'learning_rate': 9.612817089452603e-05, 'epoch': 0.68}\n",
      "{'loss': 2.3727, 'grad_norm': 1.9309970140457153, 'learning_rate': 9.572763684913217e-05, 'epoch': 0.68}\n",
      "{'loss': 1.696, 'grad_norm': 1.981310486793518, 'learning_rate': 9.53271028037383e-05, 'epoch': 0.68}\n",
      "{'loss': 2.8718, 'grad_norm': 1.9406694173812866, 'learning_rate': 9.492656875834446e-05, 'epoch': 0.68}\n",
      "{'loss': 2.6878, 'grad_norm': 1.9164139032363892, 'learning_rate': 9.45260347129506e-05, 'epoch': 0.68}\n",
      "{'loss': 2.147, 'grad_norm': 1.4863194227218628, 'learning_rate': 9.412550066755673e-05, 'epoch': 0.69}\n",
      "{'loss': 1.5902, 'grad_norm': 1.7905539274215698, 'learning_rate': 9.372496662216289e-05, 'epoch': 0.69}\n",
      "{'loss': 1.6393, 'grad_norm': 1.8061598539352417, 'learning_rate': 9.332443257676902e-05, 'epoch': 0.69}\n",
      "{'loss': 1.7945, 'grad_norm': 1.3132108449935913, 'learning_rate': 9.292389853137516e-05, 'epoch': 0.69}\n",
      "{'loss': 1.8887, 'grad_norm': 2.432404041290283, 'learning_rate': 9.25233644859813e-05, 'epoch': 0.69}\n",
      "{'loss': 2.4654, 'grad_norm': 1.8108898401260376, 'learning_rate': 9.212283044058745e-05, 'epoch': 0.69}\n",
      "{'loss': 2.4467, 'grad_norm': 1.4029667377471924, 'learning_rate': 9.172229639519359e-05, 'epoch': 0.69}\n",
      "{'loss': 1.7195, 'grad_norm': 1.6570450067520142, 'learning_rate': 9.132176234979972e-05, 'epoch': 0.7}\n",
      "{'loss': 2.0667, 'grad_norm': 1.7204616069793701, 'learning_rate': 9.092122830440588e-05, 'epoch': 0.7}\n",
      "{'loss': 2.2093, 'grad_norm': 1.4973273277282715, 'learning_rate': 9.052069425901201e-05, 'epoch': 0.7}\n",
      "{'loss': 2.062, 'grad_norm': 1.5099817514419556, 'learning_rate': 9.012016021361815e-05, 'epoch': 0.7}\n",
      "{'loss': 1.9877, 'grad_norm': 1.3723702430725098, 'learning_rate': 8.971962616822429e-05, 'epoch': 0.7}\n",
      "{'loss': 2.6144, 'grad_norm': 1.3044075965881348, 'learning_rate': 8.931909212283043e-05, 'epoch': 0.7}\n",
      "{'loss': 2.0868, 'grad_norm': 1.632226824760437, 'learning_rate': 8.891855807743658e-05, 'epoch': 0.7}\n",
      "{'loss': 1.7755, 'grad_norm': 3.005444288253784, 'learning_rate': 8.851802403204271e-05, 'epoch': 0.7}\n",
      "{'loss': 2.2518, 'grad_norm': 1.214667797088623, 'learning_rate': 8.811748998664887e-05, 'epoch': 0.71}\n",
      "{'loss': 2.0173, 'grad_norm': 1.7175055742263794, 'learning_rate': 8.7716955941255e-05, 'epoch': 0.71}\n",
      "{'loss': 2.311, 'grad_norm': 1.457818627357483, 'learning_rate': 8.731642189586114e-05, 'epoch': 0.71}\n",
      "{'loss': 2.0476, 'grad_norm': 2.0246803760528564, 'learning_rate': 8.691588785046728e-05, 'epoch': 0.71}\n",
      "{'loss': 1.9606, 'grad_norm': 1.791003942489624, 'learning_rate': 8.651535380507342e-05, 'epoch': 0.71}\n",
      "{'loss': 1.6679, 'grad_norm': 1.46999990940094, 'learning_rate': 8.611481975967957e-05, 'epoch': 0.71}\n",
      "{'loss': 2.2767, 'grad_norm': 1.5095053911209106, 'learning_rate': 8.57142857142857e-05, 'epoch': 0.71}\n",
      "{'loss': 2.1543, 'grad_norm': 2.4474856853485107, 'learning_rate': 8.531375166889186e-05, 'epoch': 0.72}\n",
      "{'loss': 1.6604, 'grad_norm': 1.608073353767395, 'learning_rate': 8.491321762349799e-05, 'epoch': 0.72}\n",
      "{'loss': 2.1726, 'grad_norm': 1.7525622844696045, 'learning_rate': 8.451268357810414e-05, 'epoch': 0.72}\n",
      "{'loss': 2.0163, 'grad_norm': 1.2087841033935547, 'learning_rate': 8.411214953271027e-05, 'epoch': 0.72}\n",
      "{'loss': 2.3138, 'grad_norm': 2.6299073696136475, 'learning_rate': 8.371161548731641e-05, 'epoch': 0.72}\n",
      "{'loss': 1.6709, 'grad_norm': 2.2563469409942627, 'learning_rate': 8.331108144192256e-05, 'epoch': 0.72}\n",
      "{'loss': 2.2037, 'grad_norm': 1.3865772485733032, 'learning_rate': 8.291054739652869e-05, 'epoch': 0.72}\n",
      "{'loss': 2.0747, 'grad_norm': 1.5799592733383179, 'learning_rate': 8.251001335113485e-05, 'epoch': 0.72}\n",
      "{'loss': 1.8398, 'grad_norm': 1.7748172283172607, 'learning_rate': 8.210947930574098e-05, 'epoch': 0.73}\n",
      "{'loss': 2.0134, 'grad_norm': 1.7546353340148926, 'learning_rate': 8.170894526034713e-05, 'epoch': 0.73}\n",
      "{'loss': 2.0358, 'grad_norm': 2.1237759590148926, 'learning_rate': 8.130841121495326e-05, 'epoch': 0.73}\n",
      "{'loss': 2.4226, 'grad_norm': 1.763066291809082, 'learning_rate': 8.09078771695594e-05, 'epoch': 0.73}\n",
      "{'loss': 2.0063, 'grad_norm': 1.5934988260269165, 'learning_rate': 8.050734312416555e-05, 'epoch': 0.73}\n",
      "{'loss': 2.1438, 'grad_norm': 1.433629035949707, 'learning_rate': 8.010680907877168e-05, 'epoch': 0.73}\n",
      "{'loss': 2.6797, 'grad_norm': 2.243304491043091, 'learning_rate': 7.970627503337784e-05, 'epoch': 0.73}\n",
      "{'loss': 1.8619, 'grad_norm': 1.5936205387115479, 'learning_rate': 7.930574098798397e-05, 'epoch': 0.74}\n",
      "{'loss': 2.3486, 'grad_norm': 1.7737177610397339, 'learning_rate': 7.890520694259012e-05, 'epoch': 0.74}\n",
      "{'loss': 2.0599, 'grad_norm': 2.3817691802978516, 'learning_rate': 7.850467289719625e-05, 'epoch': 0.74}\n",
      "{'loss': 1.9021, 'grad_norm': 1.6319186687469482, 'learning_rate': 7.81041388518024e-05, 'epoch': 0.74}\n",
      "{'loss': 2.2323, 'grad_norm': 1.3502613306045532, 'learning_rate': 7.770360480640854e-05, 'epoch': 0.74}\n",
      "{'loss': 2.2116, 'grad_norm': 1.3109443187713623, 'learning_rate': 7.730307076101467e-05, 'epoch': 0.74}\n",
      "{'loss': 2.102, 'grad_norm': 1.345585823059082, 'learning_rate': 7.690253671562083e-05, 'epoch': 0.74}\n",
      "{'loss': 2.2225, 'grad_norm': 1.2770413160324097, 'learning_rate': 7.650200267022696e-05, 'epoch': 0.74}\n",
      "{'loss': 1.859, 'grad_norm': 1.5133168697357178, 'learning_rate': 7.610146862483311e-05, 'epoch': 0.75}\n",
      "{'loss': 2.0327, 'grad_norm': 1.6526950597763062, 'learning_rate': 7.570093457943924e-05, 'epoch': 0.75}\n",
      "{'loss': 2.3896, 'grad_norm': 1.5251858234405518, 'learning_rate': 7.530040053404538e-05, 'epoch': 0.75}\n",
      "{'loss': 2.5622, 'grad_norm': 1.8650485277175903, 'learning_rate': 7.489986648865153e-05, 'epoch': 0.75}\n",
      "{'loss': 1.7182, 'grad_norm': 1.765677571296692, 'learning_rate': 7.449933244325768e-05, 'epoch': 0.75}\n",
      "{'loss': 1.6608, 'grad_norm': 1.6942288875579834, 'learning_rate': 7.409879839786381e-05, 'epoch': 0.75}\n",
      "{'loss': 2.135, 'grad_norm': 1.3236651420593262, 'learning_rate': 7.369826435246995e-05, 'epoch': 0.75}\n",
      "{'loss': 1.5484, 'grad_norm': 2.3326218128204346, 'learning_rate': 7.32977303070761e-05, 'epoch': 0.76}\n",
      "{'loss': 2.4584, 'grad_norm': 1.28165602684021, 'learning_rate': 7.289719626168223e-05, 'epoch': 0.76}\n",
      "{'loss': 2.2528, 'grad_norm': 1.6917372941970825, 'learning_rate': 7.249666221628838e-05, 'epoch': 0.76}\n",
      "{'loss': 2.447, 'grad_norm': 1.5559736490249634, 'learning_rate': 7.209612817089452e-05, 'epoch': 0.76}\n",
      "{'loss': 2.1799, 'grad_norm': 1.2614760398864746, 'learning_rate': 7.169559412550067e-05, 'epoch': 0.76}\n",
      "{'loss': 2.5221, 'grad_norm': 1.7512800693511963, 'learning_rate': 7.12950600801068e-05, 'epoch': 0.76}\n",
      "{'loss': 1.6202, 'grad_norm': 1.9787870645523071, 'learning_rate': 7.089452603471294e-05, 'epoch': 0.76}\n",
      "{'loss': 1.9186, 'grad_norm': 1.5122874975204468, 'learning_rate': 7.049399198931909e-05, 'epoch': 0.77}\n",
      "{'loss': 2.3638, 'grad_norm': 1.3996772766113281, 'learning_rate': 7.009345794392522e-05, 'epoch': 0.77}\n",
      "{'loss': 2.3732, 'grad_norm': 1.696070671081543, 'learning_rate': 6.969292389853137e-05, 'epoch': 0.77}\n",
      "{'loss': 2.3058, 'grad_norm': 1.2871348857879639, 'learning_rate': 6.929238985313751e-05, 'epoch': 0.77}\n",
      "{'loss': 2.1267, 'grad_norm': 1.689902424812317, 'learning_rate': 6.889185580774366e-05, 'epoch': 0.77}\n",
      "{'loss': 2.3711, 'grad_norm': 1.2616435289382935, 'learning_rate': 6.849132176234979e-05, 'epoch': 0.77}\n",
      "{'loss': 1.7131, 'grad_norm': 1.952995777130127, 'learning_rate': 6.809078771695593e-05, 'epoch': 0.77}\n",
      "{'loss': 1.8052, 'grad_norm': 1.8865052461624146, 'learning_rate': 6.769025367156208e-05, 'epoch': 0.77}\n",
      "{'loss': 2.1886, 'grad_norm': 2.115495443344116, 'learning_rate': 6.728971962616821e-05, 'epoch': 0.78}\n",
      "{'loss': 1.9508, 'grad_norm': 1.8524079322814941, 'learning_rate': 6.688918558077436e-05, 'epoch': 0.78}\n",
      "{'loss': 1.9324, 'grad_norm': 1.853981852531433, 'learning_rate': 6.64886515353805e-05, 'epoch': 0.78}\n",
      "{'loss': 2.7715, 'grad_norm': 1.7685019969940186, 'learning_rate': 6.608811748998665e-05, 'epoch': 0.78}\n",
      "{'loss': 1.8459, 'grad_norm': 1.219460368156433, 'learning_rate': 6.568758344459278e-05, 'epoch': 0.78}\n",
      "{'loss': 2.0932, 'grad_norm': 1.8385064601898193, 'learning_rate': 6.528704939919892e-05, 'epoch': 0.78}\n",
      "{'loss': 2.311, 'grad_norm': 1.4684526920318604, 'learning_rate': 6.488651535380507e-05, 'epoch': 0.78}\n",
      "{'loss': 2.2312, 'grad_norm': 1.5028313398361206, 'learning_rate': 6.44859813084112e-05, 'epoch': 0.79}\n",
      "{'loss': 1.8995, 'grad_norm': 1.8972651958465576, 'learning_rate': 6.408544726301735e-05, 'epoch': 0.79}\n",
      "{'loss': 2.6503, 'grad_norm': 1.567191243171692, 'learning_rate': 6.368491321762349e-05, 'epoch': 0.79}\n",
      "{'loss': 2.4036, 'grad_norm': 1.9526320695877075, 'learning_rate': 6.328437917222964e-05, 'epoch': 0.79}\n",
      "{'loss': 1.7306, 'grad_norm': 1.7941144704818726, 'learning_rate': 6.288384512683577e-05, 'epoch': 0.79}\n",
      "{'loss': 1.9302, 'grad_norm': 2.743058204650879, 'learning_rate': 6.248331108144191e-05, 'epoch': 0.79}\n",
      "{'loss': 2.0297, 'grad_norm': 1.3792303800582886, 'learning_rate': 6.208277703604806e-05, 'epoch': 0.79}\n",
      "{'loss': 2.3659, 'grad_norm': 1.8331937789916992, 'learning_rate': 6.168224299065419e-05, 'epoch': 0.79}\n",
      "{'loss': 2.1638, 'grad_norm': 1.489197015762329, 'learning_rate': 6.128170894526034e-05, 'epoch': 0.8}\n",
      "{'loss': 2.0669, 'grad_norm': 1.6005446910858154, 'learning_rate': 6.088117489986648e-05, 'epoch': 0.8}\n",
      "{'loss': 2.3998, 'grad_norm': 1.6372966766357422, 'learning_rate': 6.048064085447263e-05, 'epoch': 0.8}\n",
      "{'loss': 2.0275, 'grad_norm': 1.8519359827041626, 'learning_rate': 6.008010680907876e-05, 'epoch': 0.8}\n",
      "{'loss': 2.3134, 'grad_norm': 1.4100135564804077, 'learning_rate': 5.9679572763684906e-05, 'epoch': 0.8}\n",
      "{'loss': 2.4963, 'grad_norm': 1.9768626689910889, 'learning_rate': 5.927903871829105e-05, 'epoch': 0.8}\n",
      "{'loss': 2.2207, 'grad_norm': 0.9967519640922546, 'learning_rate': 5.887850467289719e-05, 'epoch': 0.8}\n",
      "{'loss': 2.4424, 'grad_norm': 1.307252049446106, 'learning_rate': 5.8477970627503335e-05, 'epoch': 0.81}\n",
      "{'loss': 2.42, 'grad_norm': 1.4277056455612183, 'learning_rate': 5.8077436582109474e-05, 'epoch': 0.81}\n",
      "{'loss': 2.1069, 'grad_norm': 1.5260692834854126, 'learning_rate': 5.767690253671562e-05, 'epoch': 0.81}\n",
      "{'loss': 1.4766, 'grad_norm': 1.764734148979187, 'learning_rate': 5.7276368491321764e-05, 'epoch': 0.81}\n",
      "{'loss': 1.9754, 'grad_norm': 1.730974555015564, 'learning_rate': 5.6875834445927896e-05, 'epoch': 0.81}\n",
      "{'loss': 1.6913, 'grad_norm': 1.4539309740066528, 'learning_rate': 5.647530040053404e-05, 'epoch': 0.81}\n",
      "{'loss': 1.9757, 'grad_norm': 2.020699977874756, 'learning_rate': 5.607476635514018e-05, 'epoch': 0.81}\n",
      "{'loss': 2.1521, 'grad_norm': 1.5874872207641602, 'learning_rate': 5.5674232309746326e-05, 'epoch': 0.81}\n",
      "{'loss': 2.5231, 'grad_norm': 1.2212759256362915, 'learning_rate': 5.5273698264352464e-05, 'epoch': 0.82}\n",
      "{'loss': 2.1085, 'grad_norm': 1.8788129091262817, 'learning_rate': 5.487316421895861e-05, 'epoch': 0.82}\n",
      "{'loss': 2.5805, 'grad_norm': 1.606956124305725, 'learning_rate': 5.4472630173564755e-05, 'epoch': 0.82}\n",
      "{'loss': 2.1413, 'grad_norm': 1.7097208499908447, 'learning_rate': 5.407209612817089e-05, 'epoch': 0.82}\n",
      "{'loss': 2.0248, 'grad_norm': 1.6832820177078247, 'learning_rate': 5.367156208277703e-05, 'epoch': 0.82}\n",
      "{'loss': 2.072, 'grad_norm': 1.772191047668457, 'learning_rate': 5.327102803738317e-05, 'epoch': 0.82}\n",
      "{'loss': 2.1866, 'grad_norm': 2.073803424835205, 'learning_rate': 5.2870493991989316e-05, 'epoch': 0.82}\n",
      "{'loss': 2.5175, 'grad_norm': 1.3300467729568481, 'learning_rate': 5.2469959946595455e-05, 'epoch': 0.83}\n",
      "{'loss': 2.7127, 'grad_norm': 1.4705069065093994, 'learning_rate': 5.20694259012016e-05, 'epoch': 0.83}\n",
      "{'loss': 2.482, 'grad_norm': 1.3034456968307495, 'learning_rate': 5.1668891855807746e-05, 'epoch': 0.83}\n",
      "{'loss': 2.257, 'grad_norm': 1.4323451519012451, 'learning_rate': 5.126835781041388e-05, 'epoch': 0.83}\n",
      "{'loss': 2.2227, 'grad_norm': 1.177137017250061, 'learning_rate': 5.086782376502002e-05, 'epoch': 0.83}\n",
      "{'loss': 1.5537, 'grad_norm': 1.9759318828582764, 'learning_rate': 5.046728971962616e-05, 'epoch': 0.83}\n",
      "{'loss': 2.0614, 'grad_norm': 1.6724770069122314, 'learning_rate': 5.006675567423231e-05, 'epoch': 0.83}\n",
      "{'loss': 1.8283, 'grad_norm': 2.1444036960601807, 'learning_rate': 4.9666221628838446e-05, 'epoch': 0.83}\n",
      "{'loss': 1.9441, 'grad_norm': 1.94169282913208, 'learning_rate': 4.926568758344459e-05, 'epoch': 0.84}\n",
      "{'loss': 2.4182, 'grad_norm': 1.639880657196045, 'learning_rate': 4.8865153538050736e-05, 'epoch': 0.84}\n",
      "{'loss': 1.7649, 'grad_norm': 1.5733312368392944, 'learning_rate': 4.846461949265687e-05, 'epoch': 0.84}\n",
      "{'loss': 1.8499, 'grad_norm': 1.4115139245986938, 'learning_rate': 4.8064085447263014e-05, 'epoch': 0.84}\n",
      "{'loss': 1.9701, 'grad_norm': 1.6726717948913574, 'learning_rate': 4.766355140186915e-05, 'epoch': 0.84}\n",
      "{'loss': 2.165, 'grad_norm': 2.9596214294433594, 'learning_rate': 4.72630173564753e-05, 'epoch': 0.84}\n",
      "{'loss': 1.8803, 'grad_norm': 1.5222063064575195, 'learning_rate': 4.686248331108144e-05, 'epoch': 0.84}\n",
      "{'loss': 2.6491, 'grad_norm': 1.5876981019973755, 'learning_rate': 4.646194926568758e-05, 'epoch': 0.85}\n",
      "{'loss': 2.3535, 'grad_norm': 2.174600124359131, 'learning_rate': 4.606141522029373e-05, 'epoch': 0.85}\n",
      "{'loss': 1.8435, 'grad_norm': 1.3872175216674805, 'learning_rate': 4.566088117489986e-05, 'epoch': 0.85}\n",
      "{'loss': 2.6385, 'grad_norm': 1.2775267362594604, 'learning_rate': 4.5260347129506004e-05, 'epoch': 0.85}\n",
      "{'loss': 1.9706, 'grad_norm': 2.1737060546875, 'learning_rate': 4.485981308411214e-05, 'epoch': 0.85}\n",
      "{'loss': 1.9499, 'grad_norm': 1.4623559713363647, 'learning_rate': 4.445927903871829e-05, 'epoch': 0.85}\n",
      "{'loss': 2.0325, 'grad_norm': 1.1564505100250244, 'learning_rate': 4.4058744993324434e-05, 'epoch': 0.85}\n",
      "{'loss': 1.5783, 'grad_norm': 2.105876922607422, 'learning_rate': 4.365821094793057e-05, 'epoch': 0.85}\n",
      "{'loss': 1.9929, 'grad_norm': 1.8718106746673584, 'learning_rate': 4.325767690253671e-05, 'epoch': 0.86}\n",
      "{'loss': 2.118, 'grad_norm': 1.4310543537139893, 'learning_rate': 4.285714285714285e-05, 'epoch': 0.86}\n",
      "{'loss': 1.6836, 'grad_norm': 1.9307141304016113, 'learning_rate': 4.2456608811748995e-05, 'epoch': 0.86}\n",
      "{'loss': 2.1322, 'grad_norm': 1.6908200979232788, 'learning_rate': 4.2056074766355134e-05, 'epoch': 0.86}\n",
      "{'loss': 2.1487, 'grad_norm': 1.479505181312561, 'learning_rate': 4.165554072096128e-05, 'epoch': 0.86}\n",
      "{'loss': 2.3777, 'grad_norm': 2.0351922512054443, 'learning_rate': 4.1255006675567424e-05, 'epoch': 0.86}\n",
      "{'loss': 2.5511, 'grad_norm': 1.8965919017791748, 'learning_rate': 4.085447263017356e-05, 'epoch': 0.86}\n",
      "{'loss': 1.9882, 'grad_norm': 1.9659438133239746, 'learning_rate': 4.04539385847797e-05, 'epoch': 0.87}\n",
      "{'loss': 1.9459, 'grad_norm': 1.1543164253234863, 'learning_rate': 4.005340453938584e-05, 'epoch': 0.87}\n",
      "{'loss': 2.4502, 'grad_norm': 2.8446805477142334, 'learning_rate': 3.9652870493991986e-05, 'epoch': 0.87}\n",
      "{'loss': 1.5549, 'grad_norm': 2.559751272201538, 'learning_rate': 3.9252336448598124e-05, 'epoch': 0.87}\n",
      "{'loss': 2.1848, 'grad_norm': 1.6978529691696167, 'learning_rate': 3.885180240320427e-05, 'epoch': 0.87}\n",
      "{'loss': 2.4091, 'grad_norm': 2.1111772060394287, 'learning_rate': 3.8451268357810415e-05, 'epoch': 0.87}\n",
      "{'loss': 2.5459, 'grad_norm': 1.5593003034591675, 'learning_rate': 3.8050734312416554e-05, 'epoch': 0.87}\n",
      "{'loss': 2.1183, 'grad_norm': 1.5804638862609863, 'learning_rate': 3.765020026702269e-05, 'epoch': 0.87}\n",
      "{'loss': 1.9157, 'grad_norm': 2.107069969177246, 'learning_rate': 3.724966622162884e-05, 'epoch': 0.88}\n",
      "{'loss': 2.1788, 'grad_norm': 1.2315787076950073, 'learning_rate': 3.6849132176234976e-05, 'epoch': 0.88}\n",
      "{'loss': 2.532, 'grad_norm': 1.6609286069869995, 'learning_rate': 3.6448598130841115e-05, 'epoch': 0.88}\n",
      "{'loss': 2.4215, 'grad_norm': 1.5663564205169678, 'learning_rate': 3.604806408544726e-05, 'epoch': 0.88}\n",
      "{'loss': 2.1142, 'grad_norm': 1.246769666671753, 'learning_rate': 3.56475300400534e-05, 'epoch': 0.88}\n",
      "{'loss': 2.0267, 'grad_norm': 1.3148521184921265, 'learning_rate': 3.5246995994659544e-05, 'epoch': 0.88}\n",
      "{'loss': 2.3016, 'grad_norm': 1.3098560571670532, 'learning_rate': 3.484646194926568e-05, 'epoch': 0.88}\n",
      "{'loss': 1.8756, 'grad_norm': 1.456918716430664, 'learning_rate': 3.444592790387183e-05, 'epoch': 0.89}\n",
      "{'loss': 2.3984, 'grad_norm': 1.1501855850219727, 'learning_rate': 3.404539385847797e-05, 'epoch': 0.89}\n",
      "{'loss': 2.3018, 'grad_norm': 1.9481785297393799, 'learning_rate': 3.3644859813084105e-05, 'epoch': 0.89}\n",
      "{'loss': 2.3714, 'grad_norm': 1.6406954526901245, 'learning_rate': 3.324432576769025e-05, 'epoch': 0.89}\n",
      "{'loss': 2.2619, 'grad_norm': 2.3656134605407715, 'learning_rate': 3.284379172229639e-05, 'epoch': 0.89}\n",
      "{'loss': 2.9913, 'grad_norm': 2.1202309131622314, 'learning_rate': 3.2443257676902535e-05, 'epoch': 0.89}\n",
      "{'loss': 2.207, 'grad_norm': 1.5122663974761963, 'learning_rate': 3.2042723631508673e-05, 'epoch': 0.89}\n",
      "{'loss': 2.3991, 'grad_norm': 1.6478898525238037, 'learning_rate': 3.164218958611482e-05, 'epoch': 0.89}\n",
      "{'loss': 2.2987, 'grad_norm': 1.2230721712112427, 'learning_rate': 3.124165554072096e-05, 'epoch': 0.9}\n",
      "{'loss': 2.5553, 'grad_norm': 1.2530263662338257, 'learning_rate': 3.0841121495327096e-05, 'epoch': 0.9}\n",
      "{'loss': 2.3789, 'grad_norm': 1.334032654762268, 'learning_rate': 3.044058744993324e-05, 'epoch': 0.9}\n",
      "{'loss': 2.5142, 'grad_norm': 1.6859833002090454, 'learning_rate': 3.004005340453938e-05, 'epoch': 0.9}\n",
      "{'loss': 2.3869, 'grad_norm': 1.568363904953003, 'learning_rate': 2.9639519359145526e-05, 'epoch': 0.9}\n",
      "{'loss': 2.4888, 'grad_norm': 1.2883236408233643, 'learning_rate': 2.9238985313751668e-05, 'epoch': 0.9}\n",
      "{'loss': 1.8362, 'grad_norm': 1.533097743988037, 'learning_rate': 2.883845126835781e-05, 'epoch': 0.9}\n",
      "{'loss': 2.2858, 'grad_norm': 1.3560007810592651, 'learning_rate': 2.8437917222963948e-05, 'epoch': 0.91}\n",
      "{'loss': 2.4177, 'grad_norm': 2.035802125930786, 'learning_rate': 2.803738317757009e-05, 'epoch': 0.91}\n",
      "{'loss': 2.3932, 'grad_norm': 1.492377519607544, 'learning_rate': 2.7636849132176232e-05, 'epoch': 0.91}\n",
      "{'loss': 2.0626, 'grad_norm': 1.9054527282714844, 'learning_rate': 2.7236315086782378e-05, 'epoch': 0.91}\n",
      "{'loss': 1.8741, 'grad_norm': 1.4243035316467285, 'learning_rate': 2.6835781041388516e-05, 'epoch': 0.91}\n",
      "{'loss': 2.0019, 'grad_norm': 1.6545330286026, 'learning_rate': 2.6435246995994658e-05, 'epoch': 0.91}\n",
      "{'loss': 1.5695, 'grad_norm': 1.6739405393600464, 'learning_rate': 2.60347129506008e-05, 'epoch': 0.91}\n",
      "{'loss': 1.5686, 'grad_norm': 2.9074056148529053, 'learning_rate': 2.563417890520694e-05, 'epoch': 0.91}\n",
      "{'loss': 2.7803, 'grad_norm': 1.887886881828308, 'learning_rate': 2.523364485981308e-05, 'epoch': 0.92}\n",
      "{'loss': 2.496, 'grad_norm': 2.3546836376190186, 'learning_rate': 2.4833110814419223e-05, 'epoch': 0.92}\n",
      "{'loss': 1.8005, 'grad_norm': 1.7651857137680054, 'learning_rate': 2.4432576769025368e-05, 'epoch': 0.92}\n",
      "{'loss': 2.2467, 'grad_norm': 1.4477348327636719, 'learning_rate': 2.4032042723631507e-05, 'epoch': 0.92}\n",
      "{'loss': 1.6625, 'grad_norm': 2.2677879333496094, 'learning_rate': 2.363150867823765e-05, 'epoch': 0.92}\n",
      "{'loss': 2.5165, 'grad_norm': 2.196207046508789, 'learning_rate': 2.323097463284379e-05, 'epoch': 0.92}\n",
      "{'loss': 2.0627, 'grad_norm': 1.5542689561843872, 'learning_rate': 2.283044058744993e-05, 'epoch': 0.92}\n",
      "{'loss': 2.3439, 'grad_norm': 2.4457004070281982, 'learning_rate': 2.242990654205607e-05, 'epoch': 0.93}\n",
      "{'loss': 2.1591, 'grad_norm': 1.4983521699905396, 'learning_rate': 2.2029372496662217e-05, 'epoch': 0.93}\n",
      "{'loss': 2.6139, 'grad_norm': 1.3807697296142578, 'learning_rate': 2.1628838451268355e-05, 'epoch': 0.93}\n",
      "{'loss': 2.5153, 'grad_norm': 1.4311953783035278, 'learning_rate': 2.1228304405874497e-05, 'epoch': 0.93}\n",
      "{'loss': 2.2419, 'grad_norm': 1.0757051706314087, 'learning_rate': 2.082777036048064e-05, 'epoch': 0.93}\n",
      "{'loss': 2.1816, 'grad_norm': 1.1115784645080566, 'learning_rate': 2.042723631508678e-05, 'epoch': 0.93}\n",
      "{'loss': 2.5384, 'grad_norm': 1.5453327894210815, 'learning_rate': 2.002670226969292e-05, 'epoch': 0.93}\n",
      "{'loss': 1.9278, 'grad_norm': 1.7109512090682983, 'learning_rate': 1.9626168224299062e-05, 'epoch': 0.93}\n",
      "{'loss': 1.918, 'grad_norm': 1.4899049997329712, 'learning_rate': 1.9225634178905207e-05, 'epoch': 0.94}\n",
      "{'loss': 1.9371, 'grad_norm': 1.4170618057250977, 'learning_rate': 1.8825100133511346e-05, 'epoch': 0.94}\n",
      "{'loss': 2.2453, 'grad_norm': 1.5140186548233032, 'learning_rate': 1.8424566088117488e-05, 'epoch': 0.94}\n",
      "{'loss': 1.8494, 'grad_norm': 1.5062928199768066, 'learning_rate': 1.802403204272363e-05, 'epoch': 0.94}\n",
      "{'loss': 1.6056, 'grad_norm': 1.566184639930725, 'learning_rate': 1.7623497997329772e-05, 'epoch': 0.94}\n",
      "{'loss': 1.9383, 'grad_norm': 1.4786666631698608, 'learning_rate': 1.7222963951935914e-05, 'epoch': 0.94}\n",
      "{'loss': 1.8491, 'grad_norm': 1.5237380266189575, 'learning_rate': 1.6822429906542053e-05, 'epoch': 0.94}\n",
      "{'loss': 2.3434, 'grad_norm': 2.004884719848633, 'learning_rate': 1.6421895861148195e-05, 'epoch': 0.95}\n",
      "{'loss': 2.0783, 'grad_norm': 1.378609299659729, 'learning_rate': 1.6021361815754337e-05, 'epoch': 0.95}\n",
      "{'loss': 1.9517, 'grad_norm': 1.6147496700286865, 'learning_rate': 1.562082777036048e-05, 'epoch': 0.95}\n",
      "{'loss': 2.8354, 'grad_norm': 1.6746296882629395, 'learning_rate': 1.522029372496662e-05, 'epoch': 0.95}\n",
      "{'loss': 2.4611, 'grad_norm': 1.369359016418457, 'learning_rate': 1.4819759679572763e-05, 'epoch': 0.95}\n",
      "{'loss': 2.5609, 'grad_norm': 1.914701223373413, 'learning_rate': 1.4419225634178905e-05, 'epoch': 0.95}\n",
      "{'loss': 1.6625, 'grad_norm': 1.5547090768814087, 'learning_rate': 1.4018691588785045e-05, 'epoch': 0.95}\n",
      "{'loss': 1.6717, 'grad_norm': 1.489637017250061, 'learning_rate': 1.3618157543391189e-05, 'epoch': 0.95}\n",
      "{'loss': 2.7221, 'grad_norm': 1.7075026035308838, 'learning_rate': 1.3217623497997329e-05, 'epoch': 0.96}\n",
      "{'loss': 2.5326, 'grad_norm': 2.04534912109375, 'learning_rate': 1.281708945260347e-05, 'epoch': 0.96}\n",
      "{'loss': 2.0744, 'grad_norm': 2.453122138977051, 'learning_rate': 1.2416555407209611e-05, 'epoch': 0.96}\n",
      "{'loss': 1.7029, 'grad_norm': 1.4453047513961792, 'learning_rate': 1.2016021361815753e-05, 'epoch': 0.96}\n",
      "{'loss': 2.9308, 'grad_norm': 1.6841844320297241, 'learning_rate': 1.1615487316421895e-05, 'epoch': 0.96}\n",
      "{'loss': 2.6508, 'grad_norm': 1.7272109985351562, 'learning_rate': 1.1214953271028036e-05, 'epoch': 0.96}\n",
      "{'loss': 1.6638, 'grad_norm': 1.498761534690857, 'learning_rate': 1.0814419225634178e-05, 'epoch': 0.96}\n",
      "{'loss': 2.3481, 'grad_norm': 1.5276050567626953, 'learning_rate': 1.041388518024032e-05, 'epoch': 0.97}\n",
      "{'loss': 2.5216, 'grad_norm': 1.8551034927368164, 'learning_rate': 1.001335113484646e-05, 'epoch': 0.97}\n",
      "{'loss': 2.7273, 'grad_norm': 1.7806191444396973, 'learning_rate': 9.612817089452604e-06, 'epoch': 0.97}\n",
      "{'loss': 1.9943, 'grad_norm': 1.3713123798370361, 'learning_rate': 9.212283044058744e-06, 'epoch': 0.97}\n",
      "{'loss': 1.731, 'grad_norm': 1.5061869621276855, 'learning_rate': 8.811748998664886e-06, 'epoch': 0.97}\n",
      "{'loss': 1.7781, 'grad_norm': 1.8707289695739746, 'learning_rate': 8.411214953271026e-06, 'epoch': 0.97}\n",
      "{'loss': 2.1891, 'grad_norm': 2.0390007495880127, 'learning_rate': 8.010680907877168e-06, 'epoch': 0.97}\n",
      "{'loss': 1.8407, 'grad_norm': 2.1116209030151367, 'learning_rate': 7.61014686248331e-06, 'epoch': 0.97}\n",
      "{'loss': 2.3242, 'grad_norm': 2.3000924587249756, 'learning_rate': 7.209612817089452e-06, 'epoch': 0.98}\n",
      "{'loss': 1.7915, 'grad_norm': 1.8248471021652222, 'learning_rate': 6.809078771695594e-06, 'epoch': 0.98}\n",
      "{'loss': 2.3655, 'grad_norm': 1.4985766410827637, 'learning_rate': 6.408544726301735e-06, 'epoch': 0.98}\n",
      "{'loss': 2.5254, 'grad_norm': 1.5169130563735962, 'learning_rate': 6.008010680907877e-06, 'epoch': 0.98}\n",
      "{'loss': 1.8402, 'grad_norm': 1.542155146598816, 'learning_rate': 5.607476635514018e-06, 'epoch': 0.98}\n",
      "{'loss': 2.3616, 'grad_norm': 1.5846375226974487, 'learning_rate': 5.20694259012016e-06, 'epoch': 0.98}\n",
      "{'loss': 1.6947, 'grad_norm': 2.167097806930542, 'learning_rate': 4.806408544726302e-06, 'epoch': 0.98}\n",
      "{'loss': 2.3144, 'grad_norm': 1.7538189888000488, 'learning_rate': 4.405874499332443e-06, 'epoch': 0.99}\n",
      "{'loss': 2.2537, 'grad_norm': 1.4989231824874878, 'learning_rate': 4.005340453938584e-06, 'epoch': 0.99}\n",
      "{'loss': 2.6056, 'grad_norm': 1.661249041557312, 'learning_rate': 3.604806408544726e-06, 'epoch': 0.99}\n",
      "{'loss': 2.6729, 'grad_norm': 1.5980114936828613, 'learning_rate': 3.2042723631508673e-06, 'epoch': 0.99}\n",
      "{'loss': 2.127, 'grad_norm': 1.8422138690948486, 'learning_rate': 2.803738317757009e-06, 'epoch': 0.99}\n",
      "{'loss': 2.5207, 'grad_norm': 2.7612979412078857, 'learning_rate': 2.403204272363151e-06, 'epoch': 0.99}\n",
      "{'loss': 2.0568, 'grad_norm': 1.4381245374679565, 'learning_rate': 2.002670226969292e-06, 'epoch': 0.99}\n",
      "{'loss': 2.9304, 'grad_norm': 2.0925023555755615, 'learning_rate': 1.6021361815754337e-06, 'epoch': 0.99}\n",
      "{'loss': 2.7266, 'grad_norm': 1.7814472913742065, 'learning_rate': 1.2016021361815755e-06, 'epoch': 1.0}\n",
      "{'loss': 2.7788, 'grad_norm': 2.003936529159546, 'learning_rate': 8.010680907877168e-07, 'epoch': 1.0}\n",
      "{'loss': 2.8085, 'grad_norm': 2.5739567279815674, 'learning_rate': 4.005340453938584e-07, 'epoch': 1.0}\n",
      "{'loss': 2.7454, 'grad_norm': 2.517690420150757, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b08cd0bb3446748eb406cbfc62e01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4804370403289795, 'eval_rouge1': 0.12885556092373202, 'eval_rouge2': 0.03069107851373484, 'eval_rougeL': 0.10321133376978474, 'eval_rougeLsum': 0.11766051924186802, 'eval_runtime': 150.7628, 'eval_samples_per_second': 4.968, 'eval_steps_per_second': 2.487, 'epoch': 1.0}\n",
      "{'train_runtime': 4834.9168, 'train_samples_per_second': 0.619, 'train_steps_per_second': 0.155, 'train_loss': 2.11831885743364, 'epoch': 1.0}\n",
      "CPU times: total: 7min 23s\n",
      "Wall time: 1h 20min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=749, training_loss=2.11831885743364, metrics={'train_runtime': 4834.9168, 'train_samples_per_second': 0.619, 'train_steps_per_second': 0.155, 'total_flos': 489958597260288.0, 'train_loss': 2.11831885743364, 'epoch': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "trainer.save_model('model/saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROUGE-1** measures the overlap of unigrams (individual words).\n",
    "\n",
    "**ROUGE-2** measures the overlap of bigrams (pairs of consecutive words).\n",
    "\n",
    "**ROUGE-L** measures the overlap of the longest common subsequence between the generated summary and the reference summary. This takes into account word order, but allows for gaps. Higher values ​​indicate better performance. ROUGE-L is calculated based on the similarity between the sequences, taking into account precision, recall, and the harmonic mean between them.\n",
    "\n",
    "Higher ROUGE values ​​indicate a greater similarity between the generated summary and the reference summary, which is generally interpreted as an indication of better summary quality. However, it is important to remember that no single metric can fully capture the quality of a summary, and it is useful to complement the assessment with qualitative analysis or other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy e Uso do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
